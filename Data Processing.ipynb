{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2b1de5c-9f12-45cd-88b8-87464b841d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import pickle\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f711066d-fb43-42d7-810d-3624404b69b6",
   "metadata": {},
   "source": [
    "# 1. Setup concept extractors\n",
    "\n",
    "Some options were [MetaMap](https://metamap.nlm.nih.gov/) and [spaCy](https://spacy.io/). \n",
    "\n",
    "[MetaMap](https://metamap.nlm.nih.gov/) is specific to recognizing UMLS concepts. There is a [Python wrapper](https://github.com/AnthonyMRios/pymetamap), but known to be slow and bad.\n",
    "\n",
    "[spaCy](https://spacy.io/) is a popular NLP Python package with an extensive library for named entity recognition. It has a wide variety of [extensions](https://spacy.io/universe) and models to choose from. We're going with the following.\n",
    "\n",
    "* [scispaCy](https://spacy.io/universe/project/scispacy) contains spaCy models for processing biomedical, scientific or clinical text. It seems easy to use and has a wide variety of concepts it can recognize, including UMLS, RxNorm, etc.\n",
    "\n",
    "* [negspaCy](https://spacy.io/universe/project/negspacy) identifies negations using some extension of regEx. Probably useful for things like, \"this pt is diabetic\" v. \"this pt is not diabetic.\" [todo: negation identification of medspacy might be better, https://github.com/medspacy/medspacy]\n",
    "\n",
    "* [Med7](https://github.com/kormilitzin/med7) is a model trained for recognizing entities in prescription text, e.g. identifies drug name, dosage, duration, etc., which could be useful stuff to check for conflicts. \n",
    "\n",
    "We're going with spaCy for this.. and coming up with a coherent way to integrate entities picked up by these three extensions/models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71301789-8d31-49ed-87af-bcb2215da50c",
   "metadata": {},
   "source": [
    "## i) Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aae2d503-cff3-4fe2-9fbe-e003b28420e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/opt/conda/envs/opennotes/bin/python'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys; sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33f9000b-8678-4718-88d4-e41ffefcec41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.5'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "import scispacy\n",
    "\n",
    "from pprint import pprint\n",
    "from collections import OrderedDict\n",
    "\n",
    "from spacy import displacy\n",
    "# from scispacy.abbreviation import AbbreviationDetector # UMLS already contains abbrev. detect\n",
    "from scispacy.umls_linking import UmlsEntityLinker\n",
    "\n",
    "spacy.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68518fa6-6403-4c67-ae66-0553f50ab89b",
   "metadata": {},
   "source": [
    "## ii) Setting up the model\n",
    "\n",
    "The model is used to form word/sentence embeddings for the NER task. Thus, it's important to choose model that has been tuned for our specific use case (e.g. clinical text, prescription information) so the embeddings are useful for naming the entity.\n",
    "\n",
    "[Note to self:] one potential idea to look into if we have time remaining, something about using custom model for spacy pipeline (could we do smth with the romanov models since they've been trained specifically for conflict detection?) -- https://spacy.io/usage/v3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475a77b2-fee6-456b-8413-c21f8788c584",
   "metadata": {},
   "source": [
    "### a) scispaCy\n",
    "\n",
    "For scispaCy, we set up one of their models that has been trained on biomedical data. Other models can be found [here](https://allenai.github.io/scispacy/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f41850bf-6cb2-41f0-a88e-3f0ca7398f3c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.5/en_core_sci_sm-0.2.5.tar.gz\n",
      "  Using cached https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.5/en_core_sci_sm-0.2.5.tar.gz (33.1 MB)\n",
      "Requirement already satisfied: spacy>=2.3.0 in /opt/conda/envs/opennotes/lib/python3.7/site-packages (from en-core-sci-sm==0.2.5) (2.3.5)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /opt/conda/envs/opennotes/lib/python3.7/site-packages (from spacy>=2.3.0->en-core-sci-sm==0.2.5) (0.8.2)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /opt/conda/envs/opennotes/lib/python3.7/site-packages (from spacy>=2.3.0->en-core-sci-sm==0.2.5) (1.0.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /opt/conda/envs/opennotes/lib/python3.7/site-packages (from spacy>=2.3.0->en-core-sci-sm==0.2.5) (1.0.5)\n",
      "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in /opt/conda/envs/opennotes/lib/python3.7/site-packages (from spacy>=2.3.0->en-core-sci-sm==0.2.5) (7.4.5)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/conda/envs/opennotes/lib/python3.7/site-packages (from spacy>=2.3.0->en-core-sci-sm==0.2.5) (1.20.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /opt/conda/envs/opennotes/lib/python3.7/site-packages (from spacy>=2.3.0->en-core-sci-sm==0.2.5) (0.4.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/envs/opennotes/lib/python3.7/site-packages (from spacy>=2.3.0->en-core-sci-sm==0.2.5) (3.0.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/envs/opennotes/lib/python3.7/site-packages (from spacy>=2.3.0->en-core-sci-sm==0.2.5) (2.25.1)\n",
      "Requirement already satisfied: setuptools in /opt/conda/envs/opennotes/lib/python3.7/site-packages (from spacy>=2.3.0->en-core-sci-sm==0.2.5) (49.6.0.post20210108)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/envs/opennotes/lib/python3.7/site-packages (from spacy>=2.3.0->en-core-sci-sm==0.2.5) (4.60.0)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /opt/conda/envs/opennotes/lib/python3.7/site-packages (from spacy>=2.3.0->en-core-sci-sm==0.2.5) (1.1.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/envs/opennotes/lib/python3.7/site-packages (from spacy>=2.3.0->en-core-sci-sm==0.2.5) (1.0.5)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/envs/opennotes/lib/python3.7/site-packages (from spacy>=2.3.0->en-core-sci-sm==0.2.5) (2.0.5)\n",
      "Requirement already satisfied: importlib-metadata>=0.20 in /opt/conda/envs/opennotes/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.3.0->en-core-sci-sm==0.2.5) (4.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/envs/opennotes/lib/python3.7/site-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.3.0->en-core-sci-sm==0.2.5) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/envs/opennotes/lib/python3.7/site-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.3.0->en-core-sci-sm==0.2.5) (3.4.1)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/envs/opennotes/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.0->en-core-sci-sm==0.2.5) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/envs/opennotes/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.0->en-core-sci-sm==0.2.5) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/envs/opennotes/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.0->en-core-sci-sm==0.2.5) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/opennotes/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.0->en-core-sci-sm==0.2.5) (2020.12.5)\n",
      "/opt/conda/envs/opennotes/bin/python: No module named y\n",
      "Requirement already satisfied: spacy==2.3.5 in /opt/conda/envs/opennotes/lib/python3.7/site-packages (2.3.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/envs/opennotes/lib/python3.7/site-packages (from spacy==2.3.5) (3.0.5)\n",
      "Requirement already satisfied: setuptools in /opt/conda/envs/opennotes/lib/python3.7/site-packages (from spacy==2.3.5) (49.6.0.post20210108)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/envs/opennotes/lib/python3.7/site-packages (from spacy==2.3.5) (2.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/envs/opennotes/lib/python3.7/site-packages (from spacy==2.3.5) (1.0.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/envs/opennotes/lib/python3.7/site-packages (from spacy==2.3.5) (2.25.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/envs/opennotes/lib/python3.7/site-packages (from spacy==2.3.5) (4.60.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /opt/conda/envs/opennotes/lib/python3.7/site-packages (from spacy==2.3.5) (0.4.1)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /opt/conda/envs/opennotes/lib/python3.7/site-packages (from spacy==2.3.5) (1.0.5)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /opt/conda/envs/opennotes/lib/python3.7/site-packages (from spacy==2.3.5) (1.0.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /opt/conda/envs/opennotes/lib/python3.7/site-packages (from spacy==2.3.5) (0.8.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/conda/envs/opennotes/lib/python3.7/site-packages (from spacy==2.3.5) (1.20.2)\n",
      "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in /opt/conda/envs/opennotes/lib/python3.7/site-packages (from spacy==2.3.5) (7.4.5)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /opt/conda/envs/opennotes/lib/python3.7/site-packages (from spacy==2.3.5) (1.1.3)\n",
      "Requirement already satisfied: importlib-metadata>=0.20 in /opt/conda/envs/opennotes/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy==2.3.5) (4.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/envs/opennotes/lib/python3.7/site-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy==2.3.5) (3.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/envs/opennotes/lib/python3.7/site-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy==2.3.5) (3.7.4.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/envs/opennotes/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.5) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/opennotes/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.5) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/envs/opennotes/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.5) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/envs/opennotes/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.5) (4.0.0)\n"
     ]
    }
   ],
   "source": [
    "!/opt/conda/envs/opennotes/bin/python -m pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.5/en_core_sci_sm-0.2.5.tar.gz\n",
    "!/opt/conda/envs/opennotes/bin/python -my pip uninstall negspacy\n",
    "!/opt/conda/envs/opennotes/bin/python -m pip install spacy==2.3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efb3aa0e-d096-45dc-b3ee-13baf63a3d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "sci_nlp = spacy.load(\"en_core_sci_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8bb022-64b3-4969-b880-6eb717c612a6",
   "metadata": {},
   "source": [
    "### b) Med7\n",
    "\n",
    "For Med7, we set up their model that has been trained specifically for NER of medication-related concepts: dosage, drug names, duration, form, frequency, route of administration, and strength. The model is trained on MIMIC-III, so it should work well for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9672e962-d666-466c-8b5c-4de36d4342f2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting https://www.dropbox.com/s/xbgsy6tyctvrqz3/en_core_med7_lg.tar.gz?dl=1\n",
      "  Downloading https://www.dropbox.com/s/xbgsy6tyctvrqz3/en_core_med7_lg.tar.gz?dl=1 (892.8 MB)\n",
      "\u001b[K     |█████████████████████████▍      | 709.2 MB 103.3 MB/s eta 0:00:02  |█                               | 27.1 MB 2.0 MB/s eta 0:07:06"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 892.8 MB 6.3 kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy>=2.3.2 in /opt/conda/envs/opennotes/lib/python3.7/site-packages (from en-core-med7-lg==0.0.3) (2.3.5)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /opt/conda/envs/opennotes/lib/python3.7/site-packages (from spacy>=2.3.2->en-core-med7-lg==0.0.3) (0.8.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/envs/opennotes/lib/python3.7/site-packages (from spacy>=2.3.2->en-core-med7-lg==0.0.3) (2.25.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/envs/opennotes/lib/python3.7/site-packages (from spacy>=2.3.2->en-core-med7-lg==0.0.3) (1.0.5)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /opt/conda/envs/opennotes/lib/python3.7/site-packages (from spacy>=2.3.2->en-core-med7-lg==0.0.3) (0.4.1)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /opt/conda/envs/opennotes/lib/python3.7/site-packages (from spacy>=2.3.2->en-core-med7-lg==0.0.3) (1.0.5)\n",
      "Requirement already satisfied: setuptools in /opt/conda/envs/opennotes/lib/python3.7/site-packages (from spacy>=2.3.2->en-core-med7-lg==0.0.3) (49.6.0.post20210108)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /opt/conda/envs/opennotes/lib/python3.7/site-packages (from spacy>=2.3.2->en-core-med7-lg==0.0.3) (1.1.3)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /opt/conda/envs/opennotes/lib/python3.7/site-packages (from spacy>=2.3.2->en-core-med7-lg==0.0.3) (1.0.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/envs/opennotes/lib/python3.7/site-packages (from spacy>=2.3.2->en-core-med7-lg==0.0.3) (4.60.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/envs/opennotes/lib/python3.7/site-packages (from spacy>=2.3.2->en-core-med7-lg==0.0.3) (3.0.5)\n",
      "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in /opt/conda/envs/opennotes/lib/python3.7/site-packages (from spacy>=2.3.2->en-core-med7-lg==0.0.3) (7.4.5)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/conda/envs/opennotes/lib/python3.7/site-packages (from spacy>=2.3.2->en-core-med7-lg==0.0.3) (1.20.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/envs/opennotes/lib/python3.7/site-packages (from spacy>=2.3.2->en-core-med7-lg==0.0.3) (2.0.5)\n",
      "Requirement already satisfied: importlib-metadata>=0.20 in /opt/conda/envs/opennotes/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.3.2->en-core-med7-lg==0.0.3) (4.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/envs/opennotes/lib/python3.7/site-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.3.2->en-core-med7-lg==0.0.3) (3.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/envs/opennotes/lib/python3.7/site-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.3.2->en-core-med7-lg==0.0.3) (3.7.4.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/envs/opennotes/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.2->en-core-med7-lg==0.0.3) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/envs/opennotes/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.2->en-core-med7-lg==0.0.3) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/opennotes/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.2->en-core-med7-lg==0.0.3) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/envs/opennotes/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.2->en-core-med7-lg==0.0.3) (4.0.0)\n"
     ]
    }
   ],
   "source": [
    "# installs Med7 model\n",
    "!pip install https://www.dropbox.com/s/xbgsy6tyctvrqz3/en_core_med7_lg.tar.gz?dl=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8786df8a-342d-4f5c-852c-d784902bc321",
   "metadata": {},
   "outputs": [],
   "source": [
    "med7_nlp = spacy.load(\"en_core_med7_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663387a7-1da0-4e39-80e6-534426b6a2f4",
   "metadata": {},
   "source": [
    "## iii) Adding an entity linker\n",
    "\n",
    "The EntityLinker is a spaCy component that links to a knowledge base. The linker compares words with the concepts in the specified knowledge base (e.g. scispaCy's UMLS does some form of character overlap-based nearest neighbor search, has option to resolve abbreviations first).\n",
    "\n",
    "[Note: Entities generally get resolved to a list of different entities. This [blog post](http://sujitpal.blogspot.com/2020/08/disambiguating-scispacy-umls-entities.html) describes one potential way to disambiguate this by figuring out \"most likely\" set of entities. Gonna start off with just resolving to the 1st entity tho... hopefully that's sufficient.]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cd7515-edfb-4442-885b-f52d0e5d4e27",
   "metadata": {},
   "source": [
    "### a) scispaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e405baf0-4cd0-4416-bfd1-17f600be0c93",
   "metadata": {},
   "source": [
    "#### UMLS Linker\n",
    "\n",
    "UMLS linker maps entities to the UMLS concept. Main parts we'll be interested in are: semantic type and concept (mainly the common name, maybe the CUI might become important later).\n",
    "\n",
    "* _Semantic type_ is the broader category that the entity falls under, e.g. disease, pharmacologic substance, etc. See [this](https://metamap.nlm.nih.gov/Docs/SemanticTypes_2018AB.txt) for a full list.\n",
    "\n",
    "* _Concepts_ refer to the more fundamental entity itself, e.g. pneumothorax, ventillator, etc. Many concepts can fall under a semantic type.\n",
    "\n",
    "More info on `UmlsEntityLinker` ([source code](https://github.com/allenai/scispacy/blob/4ade4ec897fa48c2ecf3187caa08a949920d126d/scispacy/linking.py#L9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc71654d-e023-42dc-b9e6-9421f40561b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/opennotes/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/opt/conda/envs/opennotes/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from scispacy.umls_linking import UmlsEntityLinker\n",
    "# import scispacy.rxnorm_linking\n",
    "\n",
    "# abbreviation_pipe = AbbreviationDetector(nlp) # automatically included with UMLS linker\n",
    "# nlp.add_pipe(abbreviation_pipe)\n",
    "linker = UmlsEntityLinker(k=10,                          # number of nearest neighbors to look up from\n",
    "                          threshold=0.7,                 # confidence threshold to be added as candidate\n",
    "                          max_entities_per_mention=1,    # number of entities returned per concept (todo: tune)\n",
    "                          filter_for_definitions=False,  # no definition is OK\n",
    "                          resolve_abbreviations=True)    # resolve abbreviations before linking\n",
    "sci_nlp.add_pipe(linker)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5a2032-9dcb-4c1d-ae02-c2785f7b7fca",
   "metadata": {},
   "source": [
    "### b) Med7 \n",
    "\n",
    "No need for entity linker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5697e483-5a37-4431-a57b-96a25a988832",
   "metadata": {},
   "source": [
    "### c) Negspacy [TODO]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb989b9c-2c0e-47ac-a74c-89237df61f6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6676f8-e249-4bda-8631-6f7dae61cb1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8906db9d-975d-4c8f-8651-684d53a4c1bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f5053d-0f84-476e-b425-9335f2fc6912",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf6c212a-fc0c-486f-b52c-6193e78842fc",
   "metadata": {},
   "source": [
    "# 2. Setup data structures\n",
    "\n",
    "## Categorizing type of conflict\n",
    "\n",
    "The first larger task is to categorize by the type of conflict to check for since our method will likely be different (at least for the rule based). We wrote up a short list [here](https://docs.google.com/document/d/1fEBk0JHeyQWshYWW5w_VTkaYyRfm9MBxJ9DAGoVa8Yw/edit?usp=sharing). \n",
    "\n",
    "To do this, we're using the semantic type that is identified by the UMLS linker. Here's a table of the semantic types we're filtering for, and which conflict they'll be used for.\n",
    "\n",
    "Here's a [full list](https://metamap.nlm.nih.gov/Docs/SemanticTypes_2018AB.txt) of semantic types. You can look up definitions of semantic types [here](http://linkedlifedata.com/resource/umls-semnetwork/T033).\n",
    "\n",
    "| Conflict | Semantic Type |\n",
    "| --- | ----------- |\n",
    "| Diagnoses-related errors | Disease or Syndrome (T047), Diagnostic Procedure(T060) |\n",
    "| Inaccurate description of medical history (symptoms) | Sign or Symptom (T184) |\n",
    "| Inaccurate description of medical history (operations) | Therapeutic or Preventive Procedure (T061) |\n",
    "| Inaccurate description of medical history (other) | [all of the above and below] |\n",
    "| Medication or allergies | Clinical Drug (T200), Pharmacologic Substance (T121) |\n",
    "| Test procedures or results | Laboratory Procedure (T059), Laboratory or Test Result (T034) | \n",
    "\n",
    "\n",
    "For clarity, the concepts we'll keep from the UMLS linker are anything falling into these semantic types (which we will then categorize by type of conflict using the table above):\n",
    "\n",
    "* T047 - Disease or Syndrome\n",
    "* T121 - Pharmacologic Substance\n",
    "* T023 - Body Part, Organ, or Organ Component\n",
    "* T061 - Therapeutic or Preventive Procedure \n",
    "* T060 - Diagnostic Procedure\n",
    "* T059 - Laboratory Procedure\n",
    "* T034 - Laboratory or Test Result \n",
    "* T184 - Sign or Symptom \n",
    "* T200 - Clinical Drug\n",
    "\n",
    "We'll store this info into a dictionary now.\n",
    "\n",
    "<!-- Some useful def's \n",
    "Finding - \n",
    "That which is discovered by direct observation or measurement of an organism attribute or condition, including the clinical history of the patient. The history of the presence of a disease is a 'Finding' and is distinguished from the disease itself.  -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "648a82cd-17ce-4623-aa5b-3927a2792a9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'T047': 'Disease or Syndrome',\n",
       " 'T121': 'Pharmacologic Substance',\n",
       " 'T023': 'Body Part, Organ, or Organ Component',\n",
       " 'T061': 'Therapeutic or Preventive Procedure',\n",
       " 'T060': 'Diagnostic Procedure',\n",
       " 'T059': 'Laboratory Procedure',\n",
       " 'T034': 'Laboratory or Test Result',\n",
       " 'T184': 'Sign or Symptom',\n",
       " 'T200': 'Clinical Drug'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEMANTIC_TYPES = ['T047', 'T121', 'T023', 'T061', 'T060', 'T059', 'T034', 'T184', 'T200']\n",
    "SEMANTIC_NAMES = ['Disease or Syndrome', 'Pharmacologic Substance', 'Body Part, Organ, or Organ Component', \\\n",
    "                  'Therapeutic or Preventive Procedure', 'Diagnostic Procedure', 'Laboratory Procedure', \\\n",
    "                  'Laboratory or Test Result', 'Sign or Symptom', 'Clinical Drug']\n",
    "SEMANTIC_TYPE_TO_NAME = dict(zip(SEMANTIC_TYPES, SEMANTIC_NAMES))\n",
    "\n",
    "SEMANTIC_TYPE_TO_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "31d7893c-015f-45bc-914f-3656f1047dae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'diagnosis': {'T047', 'T060'},\n",
       " 'med_history_symptom': {'T184'},\n",
       " 'med_history_operation': {'T061'},\n",
       " 'med_history_other': {'T023',\n",
       "  'T034',\n",
       "  'T047',\n",
       "  'T059',\n",
       "  'T060',\n",
       "  'T061',\n",
       "  'T121',\n",
       "  'T184',\n",
       "  'T200'},\n",
       " 'med_allergy': {'T121', 'T200'},\n",
       " 'test_results': {'T034', 'T059'}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CONFLICT_TO_SEMANTIC_TYPE = {\n",
    "    \"diagnosis\": {'T047', 'T060'},\n",
    "    \"med_history_symptom\": {'T184'},\n",
    "    \"med_history_operation\": {'T061'},\n",
    "    \"med_history_other\": set(SEMANTIC_TYPES),\n",
    "    \"med_allergy\": {'T200', 'T121'},\n",
    "    \"test_results\": {'T059', 'T034'}\n",
    "}\n",
    "\n",
    "CONFLICT_TO_SEMANTIC_TYPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "acd5ae61-6918-4150-8ddb-3d04eb057fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patient(object):\n",
    "    def __init__(self, hadm_id, notes_df, drug_df, lab_df, d_lab_df, \\\n",
    "                 med7_nlp, sci_nlp, umls_linker, \\\n",
    "                 physician_only=True):\n",
    "        \"\"\" Patient representation\n",
    "        \n",
    "        med7_nlp:    spacy model from Med7\n",
    "        sci_nlp:     spacy model from scispaCy\n",
    "        umls_linker: entity linker for UMLS, should already be added to sci_nlp\n",
    "        \"\"\"\n",
    "        self.hadm_id = hadm_id\n",
    "        self.physician_only = physician_only\n",
    "        \n",
    "        # this patient's data\n",
    "        self.notes_df = self.filter_notes(notes_df.loc[notes_df['HADM_ID'] == hadm_id])\n",
    "        self.drug_df  = drug_df.loc[drug_df['HADM_ID'] == hadm_id]\n",
    "        self.lab_df   = lab_df.loc[lab_df['HADM_ID'] == hadm_id]\n",
    "        \n",
    "        self.d_lab_df = d_lab_df # lab ditems df\n",
    "        \n",
    "        # spaCy models & entity linkers\n",
    "        self.med7 = med7_nlp\n",
    "        self.sci  = sci_nlp\n",
    "        self.umls = umls_linker\n",
    "        \n",
    "        # Process notes\n",
    "        notes = []\n",
    "        for row_id in pat.notes_df.ROW_ID:\n",
    "            note = Note(self, row_id)\n",
    "            notes.append(note)\n",
    "        self.notes = notes\n",
    "        \n",
    "        # todo: process labs and drugs\n",
    "        \n",
    "    def filter_notes(self, pat_notes_df):\n",
    "        if self.physician_only: pat_notes_df = self._filter_physician(pat_notes_df)\n",
    "        pat_notes_df = self._filter_duplicates(pat_notes_df)\n",
    "        \n",
    "        return pat_notes_df\n",
    "    \n",
    "    def _filter_physician(self, pat_notes_df):\n",
    "        # Filter for only physician notes\n",
    "        return pat_notes_df.loc[pat_notes_df.CATEGORY == \"Physician \"]\n",
    "        \n",
    "    def _filter_duplicates(self, pat_notes_df):\n",
    "        # Filtering out duplicate / autosave's -- only take the longest\n",
    "        for cat in pat_notes_df.CATEGORY.unique(): \n",
    "            cat_notes_df = pat_notes_df.loc[pat_notes_df.CATEGORY == cat]\n",
    "            for time in cat_notes_df.CHARTTIME.unique():\n",
    "                time_notes_df = cat_notes_df.loc[cat_notes_df.CHARTTIME == time]\n",
    "                if len(time_notes_df) > 1:\n",
    "                    # get indices of first N-1 shortest rows\n",
    "                    idx_to_drop = time_notes_df.TEXT.apply(lambda x: len(x)).sort_index().index[:-1]\n",
    "                    pat_notes_df = pat_notes_df.drop(idx_to_drop) # drop by row index\n",
    "                    \n",
    "        return pat_notes_df\n",
    "\n",
    "class Row(object):\n",
    "    def __init__(self, patient):\n",
    "        pass\n",
    "    \n",
    "    @property\n",
    "    def hadm_id(self):\n",
    "        return self.patient.hadm_id\n",
    "    \n",
    "    @property\n",
    "    def med7(self):\n",
    "        return self.patient.med7\n",
    "    \n",
    "    @property\n",
    "    def sci(self):\n",
    "        return self.patient.sci\n",
    "    \n",
    "    @property\n",
    "    def umls(self):\n",
    "        return self.patient.umls    \n",
    "\n",
    "class Note(Row):\n",
    "    def __init__(self, patient, row_id):\n",
    "        self.patient  = patient                                                   # patient this note is for\n",
    "        self.note_row = patient.notes_df.loc[patient.notes_df.ROW_ID == row_id]   # df row for this note\n",
    "        self.txt      = self.note_row.TEXT.item()                                       # note in string format\n",
    "        self.cat      = self.note_row.CATEGORY.item()                                   # note category\n",
    "        \n",
    "        # Get datetime\n",
    "        if type(self.note_row.CHARTTIME.item()) == str:\n",
    "            self.time = datetime.strptime(self.note_row.CHARTTIME.item(), \"%Y-%m-%d %H:%M:%S\")\n",
    "        elif type(self.note_row.CHARTDATE.item()) == str:\n",
    "            self.time = datetime.strptime(self.note_row.CHARTDATE.item(), \"%Y-%m-%d\")\n",
    "        else:\n",
    "            self.time = None\n",
    "            \n",
    "        # Tokenize note\n",
    "        sents = !python mimic-tokenize/heuristic-tokenize.py \"{self.txt}\"\n",
    "        sentences = sents[0].split(\", \\'\")\n",
    "#         # For python script: runs command and returns stdout as bytes, convert to utf-8, list of sentences\n",
    "#         sents = subprocess.check_output(f\"python mimic-tokenize/heuristic-tokenize.py {self.txt}\".split(\" \"))\n",
    "#         sents = sents.decode(\"utf-8\")\n",
    "#         sentences = sents.split(\", \\'\")\n",
    "\n",
    "        # Remove lab tables, remove titles\n",
    "        sentences = self._delete_copied_lab_tables(sentences)\n",
    "        sentences = self._remove_titles(sentences)\n",
    "        \n",
    "        self.sentences = sentences # todo: process each sentence\n",
    "        \n",
    "        # Process each sentence\n",
    "        sentence_reps = []\n",
    "        for idx, sent in enumerate(sentences):\n",
    "            sent_rep = Sentence(self, idx,\n",
    "                                filter_map=SEMANTIC_TYPE_TO_NAME,\n",
    "                                conflict_map=CONFLICT_TO_SEMANTIC_TYPE)\n",
    "            sentence_reps.append(sent_rep)\n",
    "            \n",
    "        self.sentence_reps = sentence_reps\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.sentence_reps[idx]\n",
    "    \n",
    "    def _diff_list(self, li1, li2):\n",
    "        return list(set(li1) - set(li2)) + list(set(li2) - set(li1))\n",
    "\n",
    "    def _delete_copied_lab_tables(self, ind_sentences):\n",
    "        # [**yyyy-mm-dd**], 02:10\n",
    "#         rgx_list = [\"[\\*\\*\\d{4}\\-\\d{1,2}\\-\\d{1,2}\\*\\*]\", \"\\d{1,2}\\-\\d{1,2}\"]\n",
    "#         rgx_list = [\"[\\*\\*[0-9]{4}-[0-9]{1,2}-[0-9]{1,2}\\*\\*] *[0-9]{1,2}-[0-9]{1,2}\"]\n",
    "#         rgx_list = [\"[\\*\\*[0-9][0-9][0-9][0-9]-[0-9][0-9]-[0-9][0-9]\\*\\*]   [0-9][0-9]-[0-9][0-9]\"]\n",
    "        rgx_list = [\"[\\*\\*[0-9][0-9][0-9][0-9]-[0-9][0-9]-[0-9][0-9]\\*\\*]\"]\n",
    "#         rgx_list = [\"[\\d{4}\\-\\d{1,2}\\-\\d{1,2}][^\\S]+\\d{1,2}\\-\\d{1,2}\"]\n",
    "        \n",
    "        delete_list = []\n",
    "        # ind_sentences is list of strings\n",
    "        for sentence in ind_sentences:\n",
    "            for rgx_match in rgx_list:\n",
    "                match = re.search(rgx_match, sentence)\n",
    "                if match and sentence not in delete_list:\n",
    "                    delete_list.append(sentence)\n",
    "        return self._diff_list(ind_sentences, delete_list)\n",
    "    \n",
    "    def _remove_titles(self, sentences):\n",
    "        \"\"\" Omits anything that has ':' in last two entries of the string. \n",
    "        e.g. \"...Results:\"\n",
    "        \"\"\"\n",
    "        return list(filter(lambda x: ':' not in x[-2:], sentences))\n",
    "        \n",
    "class Drugs(Row):\n",
    "    def __init__(self, patient):\n",
    "        pass\n",
    "\n",
    "class Labs(Row):\n",
    "    def __init__(self, patient):\n",
    "        pass\n",
    "\n",
    "class Sentence(object):\n",
    "    def __init__(self, note, sentence_idx, filter_map=None, conflict_map=None):\n",
    "        \"\"\"\n",
    "        Extracts important information and stores them as attributes. \n",
    "        \"\"\"\n",
    "        self.sentence_idx = sentence_idx\n",
    "        self.txt          = note.sentences[sentence_idx]\n",
    "\n",
    "        self.umls_cui_map = note.umls.umls.cui_to_entity # maps CUI to entity information\n",
    "        self.filter_map   = filter_map\n",
    "        self.conflict_map = conflict_map\n",
    "        self.is_filter    = (filter_map is not None)\n",
    "        self.is_conflict  = (conflict_map is not None)\n",
    "        \n",
    "        self.sci_doc  = note.sci(self.txt)\n",
    "        self.med7_doc = note.med7(self.txt)\n",
    "        \n",
    "        self.semantic_types = []\n",
    "        self.semantic_names = []  # names of categories of entities\n",
    "        self.canonical_names = [] # names of types of entities\n",
    "        self.get_umls_info()\n",
    "        \n",
    "        self.med7_entities = []   # list of tuples with (entity word, entity label), e.g. (aspirin, drug)\n",
    "        self.get_med7_info()\n",
    "        \n",
    "    def get_med7_info(self):\n",
    "        # list of tuples with (entity word, entity label), e.g. (aspirin, drug)\n",
    "        self.med7_entities = [(ent.text, ent.label_) for ent in self.med7_doc.ents]\n",
    "        \n",
    "    @property\n",
    "    def features(self):\n",
    "        \"\"\" Returns canonical names of extracted concepts, semantic type names + ID \"\"\"\n",
    "        return self.canonical_names, self.semantic_names, self.semantic_types\n",
    "\n",
    "    def get_umls_info(self):\n",
    "        for ent in self.sci_doc.ents: # extract info (umls) for each entity\n",
    "            # todo: look into this bug, ent._.umls_ents sometimes empty list\n",
    "            try:\n",
    "                cui, _ = ent._.umls_ents[0] # assuming `max_entites_per_mention=1` for now\n",
    "            except IndexError:\n",
    "                continue\n",
    "            cui_info = self.umls_cui_map[cui]\n",
    "                        \n",
    "            ent_valid_type_list = [t in self.filter_map for t in cui_info.types]\n",
    "            ent_valid_type = any(ent_valid_type_list) # checks if entity is a valid type\n",
    "            \n",
    "            if not self.is_filter or ent_valid_type: # only add to list if we're not filtering of it's valid\n",
    "                self.canonical_names.append(cui_info.canonical_name)\n",
    "                for (stype, keep) in zip(cui_info.types, ent_valid_type_list):\n",
    "                    if keep:\n",
    "                        self.semantic_types.append(stype)\n",
    "                        self.semantic_names.append(self.filter_map[stype])\n",
    "            \n",
    "        self.semantic_types = set(self.semantic_types)\n",
    "        self.semantic_names = set(self.semantic_names)\n",
    "        self.canonical_names = set(self.canonical_names)\n",
    "        \n",
    "    def similarity(self, srep):\n",
    "        \"\"\" Given another SentenceRep instance, compares similarity.\n",
    "        \n",
    "        e.g.\n",
    "        srep.similarity(srep)   # measuring similarity with itself\n",
    "        >> 1.0                  # 1.0 is maximum score\n",
    "        \"\"\"\n",
    "        return self.sci_doc.similarity(srep.doc)\n",
    "    \n",
    "    def is_ctype(self, ctype):\n",
    "        \"\"\" Given a conflict type (e.g. \"diagnosis\"),\n",
    "            returns True if this sentence falls into that category, False otherwise.\n",
    "            Returns None if conflict_map is undefined.\n",
    "        \"\"\"\n",
    "        if self.is_conflict: \n",
    "            ctype_stypes = self.conflict_map[ctype] # get list of semantic types for this conflict\n",
    "            return any([stype in ctype_stypes for stype in self.semantic_types])\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73bb66b-996b-41dd-83db-980cc2d16470",
   "metadata": {},
   "source": [
    "# 3. Load and process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fa00ab4c-6120-46bf-9bf5-d98813294c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/opennotes/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3166: DtypeWarning: Columns (4,5) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "/opt/conda/envs/opennotes/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3166: DtypeWarning: Columns (11) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# Load MIMIC tables\n",
    "notes_df  = pd.read_csv('NOTEEVENTS.csv.gz',    compression='gzip', error_bad_lines=False)\n",
    "drug_df   = pd.read_csv('PRESCRIPTIONS.csv.gz', compression='gzip', error_bad_lines=False)\n",
    "lab_df    = pd.read_csv('LABEVENTS.csv.gz',     compression='gzip', error_bad_lines=False)\n",
    "d_lab_df  = pd.read_csv('D_LABITEMS.csv.gz',    compression='gzip', error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0f2cad93-04cb-44d4-9720-50c83435590e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 8733 patients with consecutive physician notes.\n"
     ]
    }
   ],
   "source": [
    "# Load HADM ID's with consecutive physician notes\n",
    "if os.path.exists(\"hadm_ids.pkl\"):\n",
    "    with open(\"hadm_ids.pkl\", \"rb\") as f:\n",
    "        hadm_ids = pickle.load(f)\n",
    "else:\n",
    "    hadm_ids = []\n",
    "    for hadm_id in tqdm(data.HADM_ID.unique()):\n",
    "        hadm_data = data.loc[data.HADM_ID == hadm_id]\n",
    "        hadm_phys_notes = hadm_data.loc[hadm_data.CATEGORY == \"Physician \"]\n",
    "\n",
    "        if len(hadm_phys_notes) > 1:\n",
    "            hadm_ids.append(hadm_id)\n",
    "\n",
    "    with open(\"hadm_ids.pkl\", \"wb\") as f:\n",
    "        pickle.dump(hadm_ids, f)\n",
    "        \n",
    "print(f\"There are {len(hadm_ids)} patients with consecutive physician notes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "34f3a2d5-d11c-40b3-bbaf-78fee6cc1d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/opennotes/lib/python3.7/site-packages/scispacy/candidate_generation.py:283: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  extended_neighbors[empty_vectors_boolean_flags] = numpy.array(neighbors)[:-1]\n",
      "/opt/conda/envs/opennotes/lib/python3.7/site-packages/scispacy/candidate_generation.py:284: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  extended_distances[empty_vectors_boolean_flags] = numpy.array(distances)[:-1]\n",
      "/opt/conda/envs/opennotes/lib/python3.7/site-packages/scispacy/candidate_generation.py:283: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  extended_neighbors[empty_vectors_boolean_flags] = numpy.array(neighbors)[:-1]\n",
      "/opt/conda/envs/opennotes/lib/python3.7/site-packages/scispacy/candidate_generation.py:284: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  extended_distances[empty_vectors_boolean_flags] = numpy.array(distances)[:-1]\n",
      "/opt/conda/envs/opennotes/lib/python3.7/site-packages/scispacy/candidate_generation.py:283: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  extended_neighbors[empty_vectors_boolean_flags] = numpy.array(neighbors)[:-1]\n",
      "/opt/conda/envs/opennotes/lib/python3.7/site-packages/scispacy/candidate_generation.py:284: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  extended_distances[empty_vectors_boolean_flags] = numpy.array(distances)[:-1]\n",
      "/opt/conda/envs/opennotes/lib/python3.7/site-packages/scispacy/candidate_generation.py:283: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  extended_neighbors[empty_vectors_boolean_flags] = numpy.array(neighbors)[:-1]\n",
      "/opt/conda/envs/opennotes/lib/python3.7/site-packages/scispacy/candidate_generation.py:284: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  extended_distances[empty_vectors_boolean_flags] = numpy.array(distances)[:-1]\n",
      "/opt/conda/envs/opennotes/lib/python3.7/site-packages/scispacy/candidate_generation.py:283: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  extended_neighbors[empty_vectors_boolean_flags] = numpy.array(neighbors)[:-1]\n",
      "/opt/conda/envs/opennotes/lib/python3.7/site-packages/scispacy/candidate_generation.py:284: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  extended_distances[empty_vectors_boolean_flags] = numpy.array(distances)[:-1]\n",
      "/opt/conda/envs/opennotes/lib/python3.7/site-packages/scispacy/candidate_generation.py:283: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  extended_neighbors[empty_vectors_boolean_flags] = numpy.array(neighbors)[:-1]\n",
      "/opt/conda/envs/opennotes/lib/python3.7/site-packages/scispacy/candidate_generation.py:284: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  extended_distances[empty_vectors_boolean_flags] = numpy.array(distances)[:-1]\n",
      "/opt/conda/envs/opennotes/lib/python3.7/site-packages/scispacy/candidate_generation.py:283: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  extended_neighbors[empty_vectors_boolean_flags] = numpy.array(neighbors)[:-1]\n",
      "/opt/conda/envs/opennotes/lib/python3.7/site-packages/scispacy/candidate_generation.py:284: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  extended_distances[empty_vectors_boolean_flags] = numpy.array(distances)[:-1]\n",
      "/opt/conda/envs/opennotes/lib/python3.7/site-packages/scispacy/candidate_generation.py:283: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  extended_neighbors[empty_vectors_boolean_flags] = numpy.array(neighbors)[:-1]\n",
      "/opt/conda/envs/opennotes/lib/python3.7/site-packages/scispacy/candidate_generation.py:284: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  extended_distances[empty_vectors_boolean_flags] = numpy.array(distances)[:-1]\n"
     ]
    }
   ],
   "source": [
    "# test an example\n",
    "\n",
    "# Create patient instance -- processes all the data\n",
    "pat = Patient(hadm_ids[1], notes_df, drug_df, lab_df, d_lab_df, \\\n",
    "              med7_nlp, sci_nlp, linker, \\\n",
    "              physician_only=True)\n",
    "\n",
    "# note.hadm_id\n",
    "# note.txt\n",
    "# note.cat\n",
    "# note.time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c517ffac-9ac4-41c8-9c21-b2c8bc08f1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "note = pat.notes[0]\n",
    "\n",
    "for sent_rep in note.sentence_reps:\n",
    "    sent_rep.canonical_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f1c51a18-718f-4c07-85b9-6315bbcb2403",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Note at 0x7f0b258f3ed0>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for each note (same times, comparable)\n",
    "    # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86949893-e608-41c1-bc89-ae77b32a8e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get cosine sim\n",
    "vectorizer = CountVectorizer()\n",
    "corpus = list(map(lambda x: ' '.join(x), semantic_sreps_canon_names))\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "X = X.toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0dc0b6d4-6809-4a9b-aa18-4d1a4d29e65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2131-12-23 23:51:00\n",
      "2131-12-23 22:56:00\n",
      "2131-12-24 11:44:00\n",
      "2131-12-24 07:33:00\n",
      "2131-12-25 09:37:00\n",
      "2131-12-25 07:56:00\n",
      "2131-12-26 07:42:00\n",
      "2131-12-26 10:04:00\n"
     ]
    }
   ],
   "source": [
    "for note in pat.notes:\n",
    "    print(note.time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f7393864-11d7-4de5-8469-9f8daee7ff3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<__main__.Note at 0x7f0b1a142e50>,\n",
       " <__main__.Note at 0x7f0b258f1d10>,\n",
       " <__main__.Note at 0x7f0b08c12c10>,\n",
       " <__main__.Note at 0x7f0b07e5e350>,\n",
       " <__main__.Note at 0x7f0b07c36290>,\n",
       " <__main__.Note at 0x7f0b04897ed0>,\n",
       " <__main__.Note at 0x7f0b066ec190>,\n",
       " <__main__.Note at 0x7f0b041956d0>]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pat.notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b551d33e-69d6-4123-9a67-aa7214a8215e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<__main__.Sentence at 0x7f0b258f1410>,\n",
       " <__main__.Sentence at 0x7f0b258f1310>,\n",
       " <__main__.Sentence at 0x7f0b229830d0>,\n",
       " <__main__.Sentence at 0x7f0b2597d190>,\n",
       " <__main__.Sentence at 0x7f0b258ed050>,\n",
       " <__main__.Sentence at 0x7f0b0b7c47d0>,\n",
       " <__main__.Sentence at 0x7f0b2593d5d0>,\n",
       " <__main__.Sentence at 0x7f0b258da390>,\n",
       " <__main__.Sentence at 0x7f0b0a13ecd0>,\n",
       " <__main__.Sentence at 0x7f0b09b14450>,\n",
       " <__main__.Sentence at 0x7f0b1b805490>,\n",
       " <__main__.Sentence at 0x7f0b0b902a50>,\n",
       " <__main__.Sentence at 0x7f0b09a38810>,\n",
       " <__main__.Sentence at 0x7f0b0a13ee10>,\n",
       " <__main__.Sentence at 0x7f0b0ac2c7d0>,\n",
       " <__main__.Sentence at 0x7f0b0abbebd0>,\n",
       " <__main__.Sentence at 0x7f0b0a13efd0>,\n",
       " <__main__.Sentence at 0x7f0b1b20ca50>,\n",
       " <__main__.Sentence at 0x7f0b0aad3390>,\n",
       " <__main__.Sentence at 0x7f0b0aad3210>,\n",
       " <__main__.Sentence at 0x7f0b0aad33d0>,\n",
       " <__main__.Sentence at 0x7f0b09718c10>,\n",
       " <__main__.Sentence at 0x7f0b25917090>,\n",
       " <__main__.Sentence at 0x7f0b0abbea90>,\n",
       " <__main__.Sentence at 0x7f0b09718f10>,\n",
       " <__main__.Sentence at 0x7f0b25935c10>,\n",
       " <__main__.Sentence at 0x7f0b25917990>,\n",
       " <__main__.Sentence at 0x7f0b098e1910>,\n",
       " <__main__.Sentence at 0x7f0b0abbed50>,\n",
       " <__main__.Sentence at 0x7f0b09878b10>,\n",
       " <__main__.Sentence at 0x7f0b0a4a0a90>,\n",
       " <__main__.Sentence at 0x7f0b098e17d0>,\n",
       " <__main__.Sentence at 0x7f0b09a385d0>,\n",
       " <__main__.Sentence at 0x7f0b0a19c150>,\n",
       " <__main__.Sentence at 0x7f0b0a19c350>,\n",
       " <__main__.Sentence at 0x7f0b092450d0>,\n",
       " <__main__.Sentence at 0x7f0b09a38890>,\n",
       " <__main__.Sentence at 0x7f0b09245190>,\n",
       " <__main__.Sentence at 0x7f0b091cf910>,\n",
       " <__main__.Sentence at 0x7f0b0a4a0890>,\n",
       " <__main__.Sentence at 0x7f0b09fee910>,\n",
       " <__main__.Sentence at 0x7f0b0a19c3d0>,\n",
       " <__main__.Sentence at 0x7f0b091cf650>,\n",
       " <__main__.Sentence at 0x7f0b09e91ad0>,\n",
       " <__main__.Sentence at 0x7f0b09db7cd0>,\n",
       " <__main__.Sentence at 0x7f0b098e1a90>,\n",
       " <__main__.Sentence at 0x7f0b09878990>,\n",
       " <__main__.Sentence at 0x7f0b09878b90>,\n",
       " <__main__.Sentence at 0x7f0b09e24d50>,\n",
       " <__main__.Sentence at 0x7f0b09f04710>,\n",
       " <__main__.Sentence at 0x7f0b091cf790>,\n",
       " <__main__.Sentence at 0x7f0b09aa3a50>,\n",
       " <__main__.Sentence at 0x7f0b09e91990>,\n",
       " <__main__.Sentence at 0x7f0b09e91c90>,\n",
       " <__main__.Sentence at 0x7f0b0900e9d0>,\n",
       " <__main__.Sentence at 0x7f0b094eb850>,\n",
       " <__main__.Sentence at 0x7f0b09e24f90>,\n",
       " <__main__.Sentence at 0x7f0b09e24e90>,\n",
       " <__main__.Sentence at 0x7f0b09db7b50>,\n",
       " <__main__.Sentence at 0x7f0b0ac2c950>,\n",
       " <__main__.Sentence at 0x7f0b09db7d50>,\n",
       " <__main__.Sentence at 0x7f0b0ac2c690>,\n",
       " <__main__.Sentence at 0x7f0b09b14650>,\n",
       " <__main__.Sentence at 0x7f0b0a2e6190>,\n",
       " <__main__.Sentence at 0x7f0b0a831090>,\n",
       " <__main__.Sentence at 0x7f0b09b145d0>,\n",
       " <__main__.Sentence at 0x7f0b0a2e6150>,\n",
       " <__main__.Sentence at 0x7f0b0932fbd0>,\n",
       " <__main__.Sentence at 0x7f0b0ad0eb50>,\n",
       " <__main__.Sentence at 0x7f0b08b39fd0>,\n",
       " <__main__.Sentence at 0x7f0b0a6d4c90>]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pat.notes[0].sentence_reps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a82e5336-4a0b-4e3c-9485-025c82db2a70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Potassium up slightly.'\""
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_rep = pat.notes[0].sentence_reps[4]\n",
    "sent_rep.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb0dce3-e219-40e2-8d52-75a37d686067",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c45ea67-a917-4878-aa1e-d86b4c33ee5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715515ef-169b-4d6a-aafe-43ee09fdccae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c391c1-51ca-420c-a693-d746da2adcce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
