{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "948ddf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import pickle\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db31ae9",
   "metadata": {},
   "source": [
    "# 1. Setup concept extractors\n",
    "\n",
    "Some options were [MetaMap](https://metamap.nlm.nih.gov/) and [spaCy](https://spacy.io/). \n",
    "\n",
    "[MetaMap](https://metamap.nlm.nih.gov/) is specific to recognizing UMLS concepts. There is a [Python wrapper](https://github.com/AnthonyMRios/pymetamap), but known to be slow and bad.\n",
    "\n",
    "[spaCy](https://spacy.io/) is a popular NLP Python package with an extensive library for named entity recognition. It has a wide variety of [extensions](https://spacy.io/universe) and models to choose from. We're going with the following.\n",
    "\n",
    "* [scispaCy](https://spacy.io/universe/project/scispacy) contains spaCy models for processing biomedical, scientific or clinical text. It seems easy to use and has a wide variety of concepts it can recognize, including UMLS, RxNorm, etc.\n",
    "\n",
    "* [negspaCy](https://spacy.io/universe/project/negspacy) identifies negations using some extension of regEx. Probably useful for things like, \"this pt is diabetic\" v. \"this pt is not diabetic.\" [todo: negation identification of medspacy might be better, https://github.com/medspacy/medspacy]\n",
    "\n",
    "* [Med7](https://github.com/kormilitzin/med7) is a model trained for recognizing entities in prescription text, e.g. identifies drug name, dosage, duration, etc., which could be useful stuff to check for conflicts. \n",
    "\n",
    "We're going with spaCy for this.. and coming up with a coherent way to integrate entities picked up by these three extensions/models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c463c59a",
   "metadata": {},
   "source": [
    "## i) Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f1bd801",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/opt/conda/bin/python'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys; sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36870de7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2.3.5', '0.3.0')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "import scispacy\n",
    "\n",
    "from pprint import pprint\n",
    "from collections import OrderedDict\n",
    "\n",
    "from spacy import displacy\n",
    "# from scispacy.abbreviation import AbbreviationDetector # UMLS already contains abbrev. detect\n",
    "from scispacy.umls_linking import UmlsEntityLinker\n",
    "\n",
    "# should be 2.3.5 and >=0.3.0\n",
    "spacy.__version__, scispacy.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3041344",
   "metadata": {},
   "source": [
    "## ii) Setting up the model\n",
    "\n",
    "The model is used to form word/sentence embeddings for the NER task. Thus, it's important to choose model that has been tuned for our specific use case (e.g. clinical text, prescription information) so the embeddings are useful for naming the entity.\n",
    "\n",
    "[Note to self:] one potential idea to look into if we have time remaining, something about using custom model for spacy pipeline (could we do smth with the romanov models since they've been trained specifically for conflict detection?) -- https://spacy.io/usage/v3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9c1a0b",
   "metadata": {},
   "source": [
    "### a) scispaCy\n",
    "\n",
    "For scispaCy, we set up one of their models that has been trained on biomedical data. Other models can be found [here](https://allenai.github.io/scispacy/). \n",
    "\n",
    "We load two models since we will be linking different entity linkers (knowledge bases that link text to named entites) later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "3948907b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.5/en_core_sci_lg-0.2.5.tar.gz\n",
      "  Downloading https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.5/en_core_sci_lg-0.2.5.tar.gz (502.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 502.0 MB 20 kB/s s eta 0:00:01     |██████████████████████████████▌ | 478.1 MB 5.4 MB/s eta 0:00:05\n",
      "\u001b[?25hRequirement already satisfied: spacy>=2.3.0 in /opt/conda/envs/opennotes/lib/python3.7/site-packages (from en-core-sci-lg==0.2.5) (2.3.2)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /opt/conda/envs/opennotes/lib/python3.7/site-packages (from spacy>=2.3.0->en-core-sci-lg==0.2.5) (1.0.5)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /opt/conda/envs/opennotes/lib/python3.7/site-packages (from spacy>=2.3.0->en-core-sci-lg==0.2.5) (1.0.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/envs/opennotes/lib/python3.7/site-packages (from spacy>=2.3.0->en-core-sci-lg==0.2.5) (2.25.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/envs/opennotes/lib/python3.7/site-packages (from spacy>=2.3.0->en-core-sci-lg==0.2.5) (1.0.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/envs/opennotes/lib/python3.7/site-packages (from spacy>=2.3.0->en-core-sci-lg==0.2.5) (4.60.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/envs/opennotes/lib/python3.7/site-packages (from spacy>=2.3.0->en-core-sci-lg==0.2.5) (49.6.0.post20210108)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/conda/envs/opennotes/lib/python3.7/site-packages (from spacy>=2.3.0->en-core-sci-lg==0.2.5) (1.20.2)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /opt/conda/envs/opennotes/lib/python3.7/site-packages (from spacy>=2.3.0->en-core-sci-lg==0.2.5) (0.4.1)\n",
      "Requirement already satisfied: thinc==7.4.1 in /opt/conda/envs/opennotes/lib/python3.7/site-packages (from spacy>=2.3.0->en-core-sci-lg==0.2.5) (7.4.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/envs/opennotes/lib/python3.7/site-packages (from spacy>=2.3.0->en-core-sci-lg==0.2.5) (2.0.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/envs/opennotes/lib/python3.7/site-packages (from spacy>=2.3.0->en-core-sci-lg==0.2.5) (3.0.5)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /opt/conda/envs/opennotes/lib/python3.7/site-packages (from spacy>=2.3.0->en-core-sci-lg==0.2.5) (0.8.2)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /opt/conda/envs/opennotes/lib/python3.7/site-packages (from spacy>=2.3.0->en-core-sci-lg==0.2.5) (1.1.3)\n",
      "Requirement already satisfied: importlib-metadata>=0.20 in /opt/conda/envs/opennotes/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.3.0->en-core-sci-lg==0.2.5) (4.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/envs/opennotes/lib/python3.7/site-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.3.0->en-core-sci-lg==0.2.5) (3.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/envs/opennotes/lib/python3.7/site-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.3.0->en-core-sci-lg==0.2.5) (3.7.4.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/opennotes/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.0->en-core-sci-lg==0.2.5) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/envs/opennotes/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.0->en-core-sci-lg==0.2.5) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/envs/opennotes/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.0->en-core-sci-lg==0.2.5) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/envs/opennotes/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.0->en-core-sci-lg==0.2.5) (1.26.4)\n",
      "Building wheels for collected packages: en-core-sci-lg\n",
      "  Building wheel for en-core-sci-lg (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for en-core-sci-lg: filename=en_core_sci_lg-0.2.5-py3-none-any.whl size=502195831 sha256=8c5b0e3d7804073d2c54a765fed3e4a99011985e7652ca45a76ce60862019c76\n",
      "  Stored in directory: /home/jiangsharon9/.cache/pip/wheels/a5/83/74/457fe79f6f41213efa07e8b02b70387046dab5119af3e5195b\n",
      "Successfully built en-core-sci-lg\n",
      "Installing collected packages: en-core-sci-lg\n",
      "Successfully installed en-core-sci-lg-0.2.5\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.1.1 is available.\n",
      "You should consider upgrading via the '/opt/conda/envs/opennotes/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "## uncomment to install model if not already installed\n",
    "# !/opt/conda/envs/opennotes/bin/python -m pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.5/en_core_sci_sm-0.2.5.tar.gz\n",
    "!/opt/conda/envs/opennotes/bin/python -m pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.5/en_core_sci_lg-0.2.5.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "4130ce5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for umls (general biomedical concepts)\n",
    "umls_nlp   = spacy.load(\"en_core_sci_sm\")\n",
    "#umls_nlp   = spacy.load(\"en_core_sci_lg\")\n",
    "\n",
    "\n",
    "# for rxnorm (prescriptions)\n",
    "rxnorm_nlp = spacy.load(\"en_core_sci_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd645147",
   "metadata": {},
   "source": [
    "### b) Med7\n",
    "\n",
    "For Med7, we set up their model that has been trained specifically for NER of medication-related concepts: dosage, drug names, duration, form, frequency, route of administration, and strength. The model is trained on MIMIC-III, so it should work well for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e64ebfb8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting https://www.dropbox.com/s/xbgsy6tyctvrqz3/en_core_med7_lg.tar.gz?dl=1\n",
      "  Downloading https://www.dropbox.com/s/xbgsy6tyctvrqz3/en_core_med7_lg.tar.gz?dl=1 (892.8 MB)\n",
      "\u001b[K     |█████████████████               | 474.9 MB 99.3 MB/s eta 0:00:056"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |███████████████████████████▊    | 772.1 MB 85.9 MB/s eta 0:00:022"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 892.8 MB 8.0 kB/s  eta 0:00:01     |███████████████████████████████▋| 881.6 MB 86.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from en-core-med7-lg==0.0.3) (2.3.5)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from spacy>=2.3.2->en-core-med7-lg==0.0.3) (49.6.0.post20210108)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.3.2->en-core-med7-lg==0.0.3) (1.1.3)\n",
      "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.3.2->en-core-med7-lg==0.0.3) (7.4.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.3.2->en-core-med7-lg==0.0.3) (2.0.5)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.3.2->en-core-med7-lg==0.0.3) (1.0.5)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.3.2->en-core-med7-lg==0.0.3) (0.8.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.3.2->en-core-med7-lg==0.0.3) (4.60.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.3.2->en-core-med7-lg==0.0.3) (0.4.1)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.3.2->en-core-med7-lg==0.0.3) (1.0.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.3.2->en-core-med7-lg==0.0.3) (2.25.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.3.2->en-core-med7-lg==0.0.3) (1.0.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.3.2->en-core-med7-lg==0.0.3) (3.0.5)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.3.2->en-core-med7-lg==0.0.3) (1.19.5)\n",
      "Requirement already satisfied: importlib-metadata>=0.20 in /opt/conda/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.3.2->en-core-med7-lg==0.0.3) (3.10.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.3.2->en-core-med7-lg==0.0.3) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.3.2->en-core-med7-lg==0.0.3) (3.4.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.2->en-core-med7-lg==0.0.3) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.2->en-core-med7-lg==0.0.3) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.2->en-core-med7-lg==0.0.3) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.2->en-core-med7-lg==0.0.3) (4.0.0)\n"
     ]
    }
   ],
   "source": [
    "# # installs Med7 model\n",
    "!pip install https://www.dropbox.com/s/xbgsy6tyctvrqz3/en_core_med7_lg.tar.gz?dl=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25309b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "med7_nlp = spacy.load(\"en_core_med7_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65070e1",
   "metadata": {},
   "source": [
    "## iii) Adding an entity linker\n",
    "\n",
    "The EntityLinker is a spaCy component that links to a knowledge base. The linker compares words with the concepts in the specified knowledge base (e.g. scispaCy's UMLS does some form of character overlap-based nearest neighbor search, has option to resolve abbreviations first).\n",
    "\n",
    "[Note: Entities generally get resolved to a list of different entities. This [blog post](http://sujitpal.blogspot.com/2020/08/disambiguating-scispacy-umls-entities.html) describes one potential way to disambiguate this by figuring out \"most likely\" set of entities. Gonna start off with just resolving to the 1st entity tho... hopefully that's sufficient.]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ff4c28",
   "metadata": {},
   "source": [
    "### a) scispaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8485563",
   "metadata": {},
   "source": [
    "#### UMLS Linker\n",
    "\n",
    "UMLS linker maps entities to the UMLS concept. Main parts we'll be interested in are: semantic type and concept (mainly the common name, maybe the CUI might become important later).\n",
    "\n",
    "* _Semantic type_ is the broader category that the entity falls under, e.g. disease, pharmacologic substance, etc. See [this](https://metamap.nlm.nih.gov/Docs/SemanticTypes_2018AB.txt) for a full list.\n",
    "\n",
    "* _Concepts_ refer to the more fundamental entity itself, e.g. pneumothorax, ventillator, etc. Many concepts can fall under a semantic type.\n",
    "\n",
    "More info on `UmlsEntityLinker` ([source code](https://github.com/allenai/scispacy/blob/4ade4ec897fa48c2ecf3187caa08a949920d126d/scispacy/linking.py#L9))\n",
    "\n",
    "See source code for `.jsonl` file with the knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d5313c0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz not found in cache, downloading to /tmp/tmpjdeem7d2\n",
      "Finished download, copying /tmp/tmpjdeem7d2 to cache at /home/jiangsharon9/.scispacy/datasets/e9f7327283e43f0482f7c0c71b71dec278a58ccb3ffdd03c2c2350159e7ef146.f2a350ad19015b2591545f7feeed6a6d6d2fffcd635d868a5d7fc0dfc3cadfd8.tfidf_vectors_sparse.npz\n",
      "https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/data/linkers/2020-10-09/umls/nmslib_index.bin not found in cache, downloading to /tmp/tmpfpuwfipd\n",
      "Finished download, copying /tmp/tmpfpuwfipd to cache at /home/jiangsharon9/.scispacy/datasets/f48455d6c79262057cce66b4619123c2b558b21092d42fac97f47bb99a5b8f9f.dd70d3dffe7d90d7ac8914460e16a48375dab32485fb6313a34e6fbcaf53218b.nmslib_index.bin\n",
      "https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectorizer.joblib not found in cache, downloading to /tmp/tmpnyaf_1vw\n",
      "Finished download, copying /tmp/tmpnyaf_1vw to cache at /home/jiangsharon9/.scispacy/datasets/8c32f1e7ddf19ec695c321f68a71f06a191aec8efcf6b645b78fa6250d8d81d3.89019b4a62a096f33ea23677557a4cde66ebc8228f30afabac38e32f834020dc.tfidf_vectorizer.joblib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.24.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.24.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/data/linkers/2020-10-09/umls/concept_aliases.json not found in cache, downloading to /tmp/tmpmsfa3bxz\n",
      "Finished download, copying /tmp/tmpmsfa3bxz to cache at /home/jiangsharon9/.scispacy/datasets/1428ec15d3b1061731ea273c03699130b3d6b90948993e74bda66af605ff8e2a.aeb7a686c654df6bccb6c2c23d3eda3eb381daaefda4592b58158d0bee53b352.concept_aliases.json\n",
      "https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/data/kbs/2020-10-09/umls_2020_aa_cat0129.jsonl not found in cache, downloading to /tmp/tmp7jj57sql\n",
      "Finished download, copying /tmp/tmp7jj57sql to cache at /home/jiangsharon9/.scispacy/datasets/4d7fb8fcae1035d1e0a47d9072b43d5a628057d35497fbfb2499b4b7b2dd4dd7.05ec7eef12f336d4666da85b7fa69b9401883a7dd4244473f7b88b413ccbba03.umls_2020_aa_cat0129.jsonl\n"
     ]
    }
   ],
   "source": [
    "from scispacy.umls_linking import UmlsEntityLinker\n",
    "\n",
    "# abbreviation_pipe = AbbreviationDetector(nlp) # automatically included with UMLS linker\n",
    "# nlp.add_pipe(abbreviation_pipe)\n",
    "umls_linker = UmlsEntityLinker(k=10,                          # number of nearest neighbors to look up from\n",
    "                               threshold=0.7,                 # confidence threshold to be added as candidate\n",
    "                               max_entities_per_mention=1,    # number of entities returned per concept (todo: tune)\n",
    "                               filter_for_definitions=False,  # no definition is OK\n",
    "                               resolve_abbreviations=True)    # resolve abbreviations before linking\n",
    "umls_nlp.add_pipe(umls_linker)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9326738a",
   "metadata": {},
   "source": [
    "#### RxNorm Linker\n",
    "\n",
    "RxNorm linker maps entities to RxNorm, an ontology for clinical drug names. It contains about 100k concepts for normalized names for clinical drugs. It is comprised of several other drug vocabularies commonly used in pharmacy management and drug interaction, including First Databank, Micromedex, and the Gold Standard Drug Database.\n",
    "\n",
    "More info on `RxNorm` ([NIH page](https://www.nlm.nih.gov/research/umls/rxnorm/index.html), [source code](https://github.com/allenai/scispacy/blob/2290a80cfe0948e48d8ecfbd60064019d57a6874/scispacy/linking_utils.py#L120))\n",
    "\n",
    "See source code for `.jsonl` file with the knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56548dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/rxnorm/tfidf_vectors_sparse.npz not found in cache, downloading to /tmp/tmpe3vr75v3\n",
      "Finished download, copying /tmp/tmpe3vr75v3 to cache at /home/jiangsharon9/.scispacy/datasets/bda8d228cdcd3b9014f0be51cd154f7aab200b49c9edb93ffc9f3d7d6f8f7287.6a172afbb5b503f0847eded07665c53e18b0e59a4037239fd82f8d3833fc3cd5.tfidf_vectors_sparse.npz\n",
      "https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/rxnorm/nmslib_index.bin not found in cache, downloading to /tmp/tmp9xqtaio4\n",
      "Finished download, copying /tmp/tmp9xqtaio4 to cache at /home/jiangsharon9/.scispacy/datasets/aa5b2402cea8e729db0d13f67180731987f414e88662254657c6e7d1047a68c1.35fcd4b3ca0df29ece260a995bb2dcffc4580d10e8eb2f6a7d80b67c0d8bed99.nmslib_index.bin\n",
      "https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/rxnorm/tfidf_vectorizer.joblib not found in cache, downloading to /tmp/tmp9eazqqzk\n",
      "Finished download, copying /tmp/tmp9eazqqzk to cache at /home/jiangsharon9/.scispacy/datasets/5bc076acc6553adb3fcb1f6a574f74706e548bfa3beb45ded4ba3e1cb801fa91.f84a05f719449866df77e6209c5c361f515dbb836db4e66846c86ba62d9942c3.tfidf_vectorizer.joblib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.22.2.post1 when using version 0.24.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.22.2.post1 when using version 0.24.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/rxnorm/concept_aliases.json not found in cache, downloading to /tmp/tmp0z0of3js\n",
      "Finished download, copying /tmp/tmp0z0of3js to cache at /home/jiangsharon9/.scispacy/datasets/a65018bff2c6c9ef7e02f3658b2b5253fc4d52c823d985d58fcc2614ae9c5bf5.a74273b8c58718a2cd4635a4b3db50dfd129410fbbfd23fcc97c3f39314e5753.concept_aliases.json\n",
      "https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/kbs/2020-10-09/umls_2020_rxnorm.jsonl not found in cache, downloading to /tmp/tmp1phr45rn\n",
      "Finished download, copying /tmp/tmp1phr45rn to cache at /home/jiangsharon9/.scispacy/datasets/b82f1e42068c00f53c786f44bfc56353d65f7e9aec08b6b46d9c6d2c36538a76.ea8986981f7bafd0fcc8b5dc575df9adfb54145107af0e88c2ef5472b578f2b6.umls_2020_rxnorm.jsonl\n"
     ]
    }
   ],
   "source": [
    "from scispacy.linking import EntityLinker\n",
    "\n",
    "# rxnorm_linker = EntityLinker(resolve_abbreviations=True, name=\"rxnorm\")\n",
    "rxnorm_linker = EntityLinker(k=10,                          # number of nearest neighbors to look up from\n",
    "                             threshold=0.7,                 # confidence threshold to be added as candidate\n",
    "                             max_entities_per_mention=1,    # number of entities returned per concept (todo: tune)\n",
    "                             filter_for_definitions=False,  # no definition is OK\n",
    "                             resolve_abbreviations=True,    # resolve abbreviations before linking\n",
    "                             name=\"rxnorm\")                 # RxNorm ontology\n",
    "\n",
    "rxnorm_nlp.add_pipe(rxnorm_linker)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab50883",
   "metadata": {},
   "source": [
    "### b) Med7 \n",
    "\n",
    "No need for entity linker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98350eeb",
   "metadata": {},
   "source": [
    "# 2. Setup data structures\n",
    "\n",
    "## Categorizing type of conflict\n",
    "\n",
    "The first larger task is to categorize by the type of conflict to check for since our method will likely be different (at least for the rule based). We wrote up a short list [here](https://docs.google.com/document/d/1fEBk0JHeyQWshYWW5w_VTkaYyRfm9MBxJ9DAGoVa8Yw/edit?usp=sharing). \n",
    "\n",
    "To do this, we're using the semantic type that is identified by the UMLS linker. Here's a table of the semantic types we're filtering for, and which conflict they'll be used for.\n",
    "\n",
    "Here's a [full list](https://metamap.nlm.nih.gov/Docs/SemanticTypes_2018AB.txt) of semantic types. You can look up definitions of semantic types [here](http://linkedlifedata.com/resource/umls-semnetwork/T033).\n",
    "\n",
    "| Conflict | Semantic Type |\n",
    "| --- | ----------- |\n",
    "| Diagnoses-related errors | Disease or Syndrome (T047), Diagnostic Procedure(T060) |\n",
    "| Inaccurate description of medical history (symptoms) | Sign or Symptom (T184) |\n",
    "| Inaccurate description of medical history (operations) | Therapeutic or Preventive Procedure (T061) |\n",
    "| Inaccurate description of medical history (other) | [all of the above and below] |\n",
    "| Medication or allergies | Clinical Drug (T200), Pharmacologic Substance (T121) |\n",
    "| Test procedures or results | Laboratory Procedure (T059), Laboratory or Test Result (T034) | \n",
    "\n",
    "\n",
    "For clarity, the concepts we'll keep from the UMLS linker are anything falling into these semantic types (which we will then categorize by type of conflict using the table above):\n",
    "\n",
    "* T047 - Disease or Syndrome\n",
    "* T121 - Pharmacologic Substance\n",
    "* T023 - Body Part, Organ, or Organ Component\n",
    "* T061 - Therapeutic or Preventive Procedure \n",
    "* T060 - Diagnostic Procedure\n",
    "* T059 - Laboratory Procedure\n",
    "* T034 - Laboratory or Test Result \n",
    "* T184 - Sign or Symptom \n",
    "* T200 - Clinical Drug\n",
    "\n",
    "We'll store this info into a dictionary now.\n",
    "\n",
    "<!-- Some useful def's \n",
    "Finding - \n",
    "That which is discovered by direct observation or measurement of an organism attribute or condition, including the clinical history of the patient. The history of the presence of a disease is a 'Finding' and is distinguished from the disease itself.  -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16cdf505",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'T047': 'Disease or Syndrome',\n",
       " 'T121': 'Pharmacologic Substance',\n",
       " 'T023': 'Body Part, Organ, or Organ Component',\n",
       " 'T061': 'Therapeutic or Preventive Procedure',\n",
       " 'T060': 'Diagnostic Procedure',\n",
       " 'T059': 'Laboratory Procedure',\n",
       " 'T034': 'Laboratory or Test Result',\n",
       " 'T184': 'Sign or Symptom',\n",
       " 'T200': 'Clinical Drug'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEMANTIC_TYPES = ['T047', 'T121', 'T023', 'T061', 'T060', 'T059', 'T034', 'T184', 'T200']\n",
    "SEMANTIC_NAMES = ['Disease or Syndrome', 'Pharmacologic Substance', 'Body Part, Organ, or Organ Component', \\\n",
    "                  'Therapeutic or Preventive Procedure', 'Diagnostic Procedure', 'Laboratory Procedure', \\\n",
    "                  'Laboratory or Test Result', 'Sign or Symptom', 'Clinical Drug']\n",
    "SEMANTIC_TYPE_TO_NAME = dict(zip(SEMANTIC_TYPES, SEMANTIC_NAMES))\n",
    "\n",
    "SEMANTIC_TYPE_TO_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c7a5713",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'diagnosis': {'T047', 'T060'},\n",
       " 'med_history_symptom': {'T184'},\n",
       " 'med_history_operation': {'T061'},\n",
       " 'med_history_other': {'T023',\n",
       "  'T034',\n",
       "  'T047',\n",
       "  'T059',\n",
       "  'T060',\n",
       "  'T061',\n",
       "  'T121',\n",
       "  'T184',\n",
       "  'T200'},\n",
       " 'med_allergy': {'T121', 'T200'},\n",
       " 'test_results': {'T034', 'T059'}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CONFLICT_TO_SEMANTIC_TYPE = {\n",
    "    \"diagnosis\": {'T047', 'T060'},\n",
    "    \"med_history_symptom\": {'T184'},\n",
    "    \"med_history_operation\": {'T061'},\n",
    "    \"med_history_other\": set(SEMANTIC_TYPES),\n",
    "    \"med_allergy\": {'T200', 'T121'},\n",
    "    \"test_results\": {'T059', 'T034'}\n",
    "}\n",
    "\n",
    "CONFLICT_TO_SEMANTIC_TYPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094d27b1",
   "metadata": {},
   "source": [
    "# 3. Load and process MedNLI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90086708",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from data_structures import Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c03bf8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path to csv's\n",
    "train_file = \"mednli_labeled/train.csv\"\n",
    "test_file  = \"mednli_labeled/test.csv\"\n",
    "dev_file   = \"mednli_labeled/dev.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a378612e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DailyDataNull(object):\n",
    "    \"\"\" Placeholder for DailyData (e.g. Note, PrescriptionOrder, LabResults) \"\"\"\n",
    "    def __init__(self, umls, rxnorm, med7, umls_linker, rxnorm_linker):\n",
    "        self.umls   = umls\n",
    "        self.rxnorm = rxnorm\n",
    "        self.med7   = med7\n",
    "        \n",
    "        self.umls_linker   = umls_linker\n",
    "        self.rxnorm_linker = rxnorm_linker\n",
    "        \n",
    "        self.time = None\n",
    "        \n",
    "class MedNLI(Dataset):\n",
    "    \"\"\" MedNLI dataset. \"\"\"\n",
    "    def __init__(self, data_filepath):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_filepath (string): Path to the csv file with labeled data.\n",
    "        \"\"\"\n",
    "        self.df = pd.read_csv(data_filepath)\n",
    "        self.nullnote = DailyDataNull(umls_nlp, rxnorm_nlp, med7_nlp,\n",
    "                                      umls_linker, rxnorm_linker)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item  = self.df.iloc[idx]\n",
    "        label = item['label']\n",
    "        \n",
    "        # create the sentences\n",
    "        sentence1 = Sentence(self.nullnote, None,\n",
    "                             filter_map=SEMANTIC_TYPE_TO_NAME,\n",
    "                             conflict_map=CONFLICT_TO_SEMANTIC_TYPE,\n",
    "                             sentence=item['sentence 1'])\n",
    "        sentence2 = Sentence(self.nullnote, None,\n",
    "                             filter_map=SEMANTIC_TYPE_TO_NAME,\n",
    "                             conflict_map=CONFLICT_TO_SEMANTIC_TYPE,\n",
    "                             sentence=item['sentence 2'])\n",
    "        \n",
    "        return (sentence1, sentence2), label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8eea824",
   "metadata": {},
   "source": [
    "##Example loading training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f496577",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MedNLI(train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "38eb4307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: He received 2U PRBC, underwent tagged RBC scan, transferred to [**Hospital1 22**].\n",
      "Sentence 2:  The patient is anemic. \n",
      "Contradiction (0=no, 1=yes)? 0\n"
     ]
    }
   ],
   "source": [
    "# Get 1st pair\n",
    "(s1, s2), label = train_dataset[7484]\n",
    "\n",
    "print(f\"Sentence 1: {s1.txt}\\nSentence 2: {s2.txt}\\nContradiction (0=no, 1=yes)? {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0740df9b",
   "metadata": {},
   "source": [
    "You can also process all the sentences upfront and save."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ebe59cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "94550e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7488/7488 [36:31<00:00,  3.42it/s]\n"
     ]
    }
   ],
   "source": [
    "all_sentence1 = []\n",
    "all_sentence2 = []\n",
    "all_labels    = []\n",
    "for i in tqdm(range(len(train_dataset))):\n",
    "    (s1, s2), label = train_dataset[i]\n",
    "    \n",
    "    all_sentence1.append(s1)\n",
    "    all_sentence2.append(s2)\n",
    "    all_labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d6effeda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Labs were notable for Cr 1.7 (baseline 0.5 per old records) and lactate 2.4.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_sentence1[0].txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b834f797",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't pickle nmslib.dist.FloatIndex objects",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-e3ee9833ea99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0msave_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_sentence1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_sentence2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-37-e3ee9833ea99>\u001b[0m in \u001b[0;36msave_data\u001b[0;34m(all_sentence1, all_sentence2, all_labels)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msave_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_sentence1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_sentence2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sentences.pickle\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_sentence1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_sentence2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't pickle nmslib.dist.FloatIndex objects"
     ]
    }
   ],
   "source": [
    "# save all_sentence_1, all_sentence2, all_labels\n",
    "\n",
    "import pickle\n",
    "\n",
    "def load_data():\n",
    "    try:\n",
    "        with open(\"sentences.pickle\", \"rb\") as f:\n",
    "            all_sentence1, all_sentence2, all_labels = pickle.load(f)\n",
    "    except:\n",
    "        all_sentence1, all_sentence2, all_labels = [], []\n",
    "    return all_sentence1, all_sentence2, all_labels\n",
    "\n",
    "def save_data(all_sentence1, all_sentence2, all_labels):\n",
    "    with open(\"sentences.pickle\", \"wb\") as f:\n",
    "        pickle.dump((all_sentence1, all_sentence2, all_labels), f)\n",
    "   \n",
    "        \n",
    "save_data(all_sentence1, all_sentence2, all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db438f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "425150f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't pickle nmslib.dist.FloatIndex objects",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-179a0363687f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# write a file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"example.dat\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_sentence1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_sentence2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't pickle nmslib.dist.FloatIndex objects"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# write a file\n",
    "f = open(\"example.dat\", \"wb\")\n",
    "pickle.dump(all_sentence1, f)\n",
    "pickle.dump(all_sentence2, f)\n",
    "pickle.dump(all_labels, f)\n",
    "f.close()\n",
    "\n",
    "f = open(\"example.dat\", \"rb\")\n",
    "all_sentence1 = pickle.load(f)\n",
    "all_sentence2 = pickle.load(f)\n",
    "all_labels = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "303a6d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dill\n",
      "  Downloading dill-0.3.3-py2.py3-none-any.whl (81 kB)\n",
      "\u001b[K     |████████████████████████████████| 81 kB 4.2 MB/s eta 0:00:011\n",
      "\u001b[?25hInstalling collected packages: dill\n",
      "Successfully installed dill-0.3.3\n"
     ]
    }
   ],
   "source": [
    "!pip install dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f0c97576",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-24ecf751bc42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#[<listiterator object at 0x10b0e48d0>]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# note that nothing inside of the iterator is unpicklable!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdill\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbaditems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/dill/detect.py\u001b[0m in \u001b[0;36mbaditems\u001b[0;34m(obj, exact, safe)\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'values'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0m_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# can't use a set, as items may be unhashable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m     \u001b[0;34m[\u001b[0m\u001b[0m_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbadobjects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mexact\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msafe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_obj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_obj\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/dill/detect.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'values'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0m_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# can't use a set, as items may be unhashable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m     \u001b[0;34m[\u001b[0m\u001b[0m_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbadobjects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mexact\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msafe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_obj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_obj\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/dill/detect.py\u001b[0m in \u001b[0;36mbadobjects\u001b[0;34m(obj, depth, exact, safe)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mdill\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mpickles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mexact\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msafe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     return dict(((attr, badobjects(getattr(obj,attr),depth-1,exact,safe)) \\\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/dill/_dill.py\u001b[0m in \u001b[0;36mpickles\u001b[0;34m(obj, exact, safe, **kwds)\u001b[0m\n\u001b[1;32m   1479\u001b[0m         \u001b[0mexceptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPicklingError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnpicklingError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1480\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1481\u001b[0;31m         \u001b[0mpik\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1482\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1483\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpik\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/dill/_dill.py\u001b[0m in \u001b[0;36mcopy\u001b[0;34m(obj, *args, **kwds)\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;34m\"\"\"use pickling to 'copy' an object\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0mignore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbyref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m#, strictio=None):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/dill/_dill.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(obj, protocol, byref, fmode, recurse, **kwds)\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0;34m\"\"\"pickle an object to a string\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStringIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m     \u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbyref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#, strictio)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/dill/_dill.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(obj, file, protocol, byref, fmode, recurse, **kwds)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0m_kwds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0m_kwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbyref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrecurse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m     \u001b[0mPickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0m_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/dill/_dill.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    452\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mPicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m             \u001b[0mStockPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m         \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# clear record of 'recursion-sensitive' pickled objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproto\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_framing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSTOP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_framing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;31m# Save the reduce() output and finally memoize the object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_reduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mrv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpersistent_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36msave_reduce\u001b[0;34m(self, func, args, state, listitems, dictitems, obj)\u001b[0m\n\u001b[1;32m    660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 662\u001b[0;31m             \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    663\u001b[0m             \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBUILD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    502\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Call unbound method with explicit self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/dill/_dill.py\u001b[0m in \u001b[0;36msave_module_dict\u001b[0;34m(pickler, obj)\u001b[0m\n\u001b[1;32m    939\u001b[0m             \u001b[0;31m# we only care about session the first pass thru\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    940\u001b[0m             \u001b[0mpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 941\u001b[0;31m         \u001b[0mStockPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    942\u001b[0m         \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"# D2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    943\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36msave_dict\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemoize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_setitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m     \u001b[0mdispatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msave_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36m_batch_setitems\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m    883\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m                     \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m                     \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m                 \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSETITEMS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;31m# Save the reduce() output and finally memoize the object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_reduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mrv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpersistent_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36msave_reduce\u001b[0;34m(self, func, args, state, listitems, dictitems, obj)\u001b[0m\n\u001b[1;32m    660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 662\u001b[0;31m             \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    663\u001b[0m             \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBUILD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    502\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Call unbound method with explicit self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/dill/_dill.py\u001b[0m in \u001b[0;36msave_module_dict\u001b[0;34m(pickler, obj)\u001b[0m\n\u001b[1;32m    939\u001b[0m             \u001b[0;31m# we only care about session the first pass thru\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    940\u001b[0m             \u001b[0mpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 941\u001b[0;31m         \u001b[0mStockPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    942\u001b[0m         \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"# D2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    943\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36msave_dict\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemoize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_setitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m     \u001b[0mdispatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msave_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36m_batch_setitems\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m    883\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m                     \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m                     \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m                 \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSETITEMS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;31m# Save the reduce() output and finally memoize the object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_reduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mrv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpersistent_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36msave_reduce\u001b[0;34m(self, func, args, state, listitems, dictitems, obj)\u001b[0m\n\u001b[1;32m    660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 662\u001b[0;31m             \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    663\u001b[0m             \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBUILD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    502\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Call unbound method with explicit self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/dill/_dill.py\u001b[0m in \u001b[0;36msave_module_dict\u001b[0;34m(pickler, obj)\u001b[0m\n\u001b[1;32m    939\u001b[0m             \u001b[0;31m# we only care about session the first pass thru\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    940\u001b[0m             \u001b[0mpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 941\u001b[0;31m         \u001b[0mStockPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    942\u001b[0m         \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"# D2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    943\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36msave_dict\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemoize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_setitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m     \u001b[0mdispatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msave_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36m_batch_setitems\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m    883\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m                     \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m                     \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m                 \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSETITEMS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;31m# Save the reduce() output and finally memoize the object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_reduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mrv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpersistent_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36msave_reduce\u001b[0;34m(self, func, args, state, listitems, dictitems, obj)\u001b[0m\n\u001b[1;32m    636\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m             \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m             \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m             \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mREDUCE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    502\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Call unbound method with explicit self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36msave_tuple\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    787\u001b[0m         \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMARK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0melement\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m             \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;31m# Save the reduce() output and finally memoize the object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_reduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mrv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpersistent_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36msave_reduce\u001b[0;34m(self, func, args, state, listitems, dictitems, obj)\u001b[0m\n\u001b[1;32m    660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 662\u001b[0;31m             \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    663\u001b[0m             \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBUILD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    502\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Call unbound method with explicit self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/dill/_dill.py\u001b[0m in \u001b[0;36msave_module_dict\u001b[0;34m(pickler, obj)\u001b[0m\n\u001b[1;32m    939\u001b[0m             \u001b[0;31m# we only care about session the first pass thru\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    940\u001b[0m             \u001b[0mpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 941\u001b[0;31m         \u001b[0mStockPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    942\u001b[0m         \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"# D2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    943\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36msave_dict\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemoize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_setitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m     \u001b[0mdispatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msave_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36m_batch_setitems\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m    888\u001b[0m                 \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m                 \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 890\u001b[0;31m                 \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    891\u001b[0m                 \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSETITEM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m             \u001b[0;31m# else tmp is empty, and we're done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;31m# Save the reduce() output and finally memoize the object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_reduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mrv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpersistent_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36msave_reduce\u001b[0;34m(self, func, args, state, listitems, dictitems, obj)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdictitems\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 659\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_setitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36m_batch_setitems\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m    883\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m                     \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m                     \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m                 \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSETITEMS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;31m# Save the reduce() output and finally memoize the object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_reduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mrv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpersistent_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36msave_reduce\u001b[0;34m(self, func, args, state, listitems, dictitems, obj)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdictitems\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 659\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_setitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36m_batch_setitems\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m    882\u001b[0m                 \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMARK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m                     \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m                     \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m                 \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSETITEMS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    502\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Call unbound method with explicit self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36msave_long\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    699\u001b[0m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLONG1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"<B\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mencoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLONG4\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"<i\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mencoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_frame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrite_large_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpayload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import dill\n",
    "x = all_sentence1\n",
    "d = {'x':x}\n",
    "# we check for unpicklable items in d (i.e. the iterator x)\n",
    "dill.detect.baditems(d)\n",
    "#[<listiterator object at 0x10b0e48d0>]\n",
    "# note that nothing inside of the iterator is unpicklable!\n",
    "dill.detect.baditems(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691f01d0",
   "metadata": {},
   "source": [
    "What can we access from sentence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "146f8e3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Scanning'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# UMLS and RxNorm concepts \n",
    "s1.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "06588a67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2U', 'DOSAGE'), ('PRBC', 'DRUG')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Med7 (prescription) entities\n",
    "s1.med7_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "902d2885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pregnancy was complicated by spotting at 18 weeks and 26 weeks.\n"
     ]
    }
   ],
   "source": [
    "# \"Doc\" outputs from spacy are also saved, which can be useful (I think you have some exploration on this already)\n",
    "s1.umls_doc    # \"Doc\" output for UMLS \n",
    "s1.rxnorm_doc  # \"Doc\" output for RxNorm \n",
    "s1.med7_doc    # \"Doc\" output for Med7\n",
    "\n",
    "#displacy.render(s1.umls_doc)\n",
    "#displacy.render(s1.rxnorm_doc)\n",
    "#print(s1.rxnorm_doc)\n",
    "#print(s1.med7_doc)\n",
    "#displacy.render(s1.med7_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "16b7f6ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original word: RBC\n",
      "CUI: C0014792\n",
      "UMLS Info: CUI: C0014792, Name: Erythrocytes\n",
      "Definition: Red blood cells. Mature erythrocytes are non-nucleated, biconcave disks containing HEMOGLOBIN whose function is to transport OXYGEN.\n",
      "TUI(s): T025\n",
      "Aliases (abbreviated, total: 48): \n",
      "\t Blood Cells, Red, Red Blood Cells, Blood erythrocytic cell, Blood Erythrocyte, Red Cell, Erythrocytic Cells, rbcs, Marrow erythrocyte, red blood cell, Blood red cell\n",
      "*********************\n",
      "Original word: scan\n",
      "CUI: C0441633\n",
      "UMLS Info: CUI: C0441633, Name: Scanning\n",
      "Definition: A picture of structures inside the body. Scans often used in diagnosing, staging, and monitoring disease include liver scans, bone scans, and computed tomography (CT) or computerized axial tomography (CAT) scans and magnetic resonance imaging (MRI) scans. In liver scanning and bone scanning, radioactive substances that are injected into the bloodstream collect in these organs. A scanner that detects the radiation is used to create pictures. In CT scanning, an x-ray machine linked to a computer is used to produce detailed pictures of organs inside the body. MRI scans use a large magnet connected to a computer to create pictures of areas inside the body.\n",
      "TUI(s): T060\n",
      "Aliases: (total: 8): \n",
      "\t scanned, diagnostic scanning, Scan, Scans, diagnostic scan, scan, scans, scanning\n",
      "*********************\n"
     ]
    }
   ],
   "source": [
    "# How can we tell which word the concept came from?\n",
    "# This is a slightly modified version of what we do in \n",
    "# Data.get_umls_info() and Data.get_rxnorm_info(). \n",
    "# Check these functions in data_structures.py \n",
    "# for more info on how to access other info.\n",
    "\n",
    "sentence_entities = s1.umls_doc.ents\n",
    "umls_cui_map = umls_linker.umls.cui_to_entity # maps CUI to UMLS knowledge base\n",
    "for ent in sentence_entities: # extract info (umls) for each entity\n",
    "    try:\n",
    "        cui, _ = ent._.umls_ents[0] # assuming `max_entites_per_mention=1` for now\n",
    "    except IndexError:\n",
    "        continue\n",
    "    cui_info = umls_cui_map[cui]\n",
    "    \n",
    "    print(f\"Original word: {ent}\\nCUI: {cui}\\nUMLS Info: {cui_info}\")\n",
    "    print(\"*********************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1e51f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8357170a",
   "metadata": {},
   "source": [
    "## Feature generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "22404e48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef check_if_number_neg_equal_rxnorm(s1, s2):\\n    sent_doc_1_rxnorm = s1.rxnorm_doc    # \"Doc\" output for rxnorm \\n    sent_doc_2_rxnorm = s2.rxnorm_doc    # \"Doc\" output for rxnorm\\n\\n    negation_tokens_1 = [tok for tok in sent_doc_1_rxnorm if tok.dep_ == \\'neg\\']\\n    negation_tokens_2 = [tok for tok in sent_doc_2_rxnorm if tok.dep_ == \\'neg\\']\\n\\n    if len(negation_tokens_1) != len(negation_tokens_2):\\n        return 1\\n    else:\\n        return 0\\n    \\n    \\ndef check_if_number_neg_equal_med7(s1, s2):\\n    sent_doc_1_umls = s1.med7_doc    # \"Doc\" output for Med7\\n    sent_doc_2_umls = s2.med7_doc    # \"Doc\" output for Med7\\n\\n    negation_tokens_1 = [tok for tok in sent_doc_1_umls if tok.dep_ == \\'neg\\']\\n    negation_tokens_2 = [tok for tok in sent_doc_2_umls if tok.dep_ == \\'neg\\']\\n\\n    if len(negation_tokens_1) != len(negation_tokens_2):\\n        return 1\\n    else:\\n        return 0\\n'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if number of neg tokens is equal, return 0; otherwise, return 1\n",
    "def check_if_number_neg_equal_umls(s1, s2):\n",
    "    sent_doc_1_umls = s1.umls_doc    # \"Doc\" output for UMLS \n",
    "    sent_doc_2_umls = s2.umls_doc    # \"Doc\" output for UMLS\n",
    "    \n",
    "    negation_tokens_1 = [tok for tok in sent_doc_1_umls if tok.dep_ == 'neg']\n",
    "    negation_tokens_2 = [tok for tok in sent_doc_2_umls if tok.dep_ == 'neg']\n",
    "\n",
    "    if len(negation_tokens_1) != len(negation_tokens_2):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "\"\"\"\n",
    "def check_if_number_neg_equal_rxnorm(s1, s2):\n",
    "    sent_doc_1_rxnorm = s1.rxnorm_doc    # \"Doc\" output for rxnorm \n",
    "    sent_doc_2_rxnorm = s2.rxnorm_doc    # \"Doc\" output for rxnorm\n",
    "\n",
    "    negation_tokens_1 = [tok for tok in sent_doc_1_rxnorm if tok.dep_ == 'neg']\n",
    "    negation_tokens_2 = [tok for tok in sent_doc_2_rxnorm if tok.dep_ == 'neg']\n",
    "\n",
    "    if len(negation_tokens_1) != len(negation_tokens_2):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "    \n",
    "def check_if_number_neg_equal_med7(s1, s2):\n",
    "    sent_doc_1_umls = s1.med7_doc    # \"Doc\" output for Med7\n",
    "    sent_doc_2_umls = s2.med7_doc    # \"Doc\" output for Med7\n",
    "\n",
    "    negation_tokens_1 = [tok for tok in sent_doc_1_umls if tok.dep_ == 'neg']\n",
    "    negation_tokens_2 = [tok for tok in sent_doc_2_umls if tok.dep_ == 'neg']\n",
    "\n",
    "    if len(negation_tokens_1) != len(negation_tokens_2):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d176aff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# could do word vector for spacy? https://www.shanelynn.ie/word-embeddings-in-python-with-spacy-and-gensim/ \n",
    "\n",
    "def create_sentence_encoding(sentence_1, sentence_2):\n",
    "    CountVec = CountVectorizer(ngram_range=(1,1), stop_words='english') # to use bigrams, ngram_range=(2,2)\n",
    "\n",
    "    # transform\n",
    "    Count_data = CountVec.fit_transform([sentence_1, sentence_2])\n",
    "\n",
    "    # create dataframe\n",
    "    cv_dataframe = pd.DataFrame(Count_data.toarray(), columns=CountVec.get_feature_names())\n",
    "    return cv_dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6a10f99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "dependency words as bag of words\n",
    "\"\"\"\n",
    "def create_dep_encoding(s1, s2):\n",
    "\n",
    "    #sent_doc_1 = nlp(sentence_1)\n",
    "    #sent_doc_2 = nlp(sentence_2)\n",
    "    \n",
    "    sent_doc_1 = s1.umls_doc\n",
    "    sent_doc_2 = s2.umls_doc\n",
    "\n",
    "    dep_child_1 = []\n",
    "    dep_child_2 = []\n",
    "\n",
    "    for token in sent_doc_1:\n",
    "        if token.dep_ == \"ROOT\": # if there are multiple roots?\n",
    "            index_to_check = token.i\n",
    "            dep_list_1 = [token.text for token in sent_doc_1[index_to_check].children if token.dep_ != \"punct\"]\n",
    "            dep_child_1.append(sent_doc_1[index_to_check].text)\n",
    "            dep_child_1.extend(dep_list_1)\n",
    "\n",
    "    for token in sent_doc_2:\n",
    "        if token.dep_ == \"ROOT\":\n",
    "            index_to_check = token.i\n",
    "            dep_list_2 = [token.text for token in sent_doc_2[index_to_check].children if token.dep_ != \"punct\"]\n",
    "            dep_child_2.append(sent_doc_2[index_to_check].text)\n",
    "            dep_child_2.extend(dep_list_2)\n",
    "\n",
    "    dep_sent_1 = list_to_string(dep_child_1)\n",
    "    dep_sent_2 = list_to_string(dep_child_2)\n",
    "\n",
    "    #return dep_sent_1, dep_sent_2\n",
    "    dep_doc_1 = umls_nlp(dep_sent_1)\n",
    "    dep_doc_2 = umls_nlp(dep_sent_2)\n",
    "    similarity = dep_doc_1.similarity(dep_doc_2)\n",
    "    return similarity\n",
    "\n",
    "def list_to_string(list1):\n",
    "    str1 = \"\"\n",
    "    for element in list1:\n",
    "        str1 += \" \" + element\n",
    "\n",
    "    return str1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "328de992",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Given Sentence instances s1, s2\n",
    "check if they share a shared concept feature (UMLS and RxNorm concepts) \n",
    "if they do not share a concept, return 1; otherwise, return 0\n",
    "\"\"\"\n",
    "def check_shared_feature_umls(s1,s2):\n",
    "    #{'Scanning'}\n",
    "    s1_features = s1.features\n",
    "    s2_features = s2.features\n",
    "    \n",
    "    # check if share feature in common\n",
    "    shared_feature = list(set(s1_features) & set(s2_features))\n",
    "    \n",
    "    # do not share concept\n",
    "    if shared_feature != []:\n",
    "        return 1\n",
    "    else: \n",
    "        return 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2f6bb9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Med7 (prescription) entities\n",
    "# if not talking about same DRUG -> return 0\n",
    "# if same DRUG but different other info -> return 1\n",
    "def check_shared_feature_med7(s1,s2):\n",
    "    \n",
    "    # [('2U', 'DOSAGE'), ('PRBC', 'DRUG')]\n",
    "    s1_features = s1.med7_entities\n",
    "    s2_features = s2.med7_entities\n",
    "    \n",
    "    # get drug names for each sentence\n",
    "    s1_names = [name for (name, word_type) in s1_features if word_type == \"DRUG\"]\n",
    "    s2_names = [name for (name, word_type) in s2_features if word_type == \"DRUG\"]\n",
    "    \n",
    "    # check if drug name is in common\n",
    "    shared_drug = list(set(s1_names) & set(s2_names))\n",
    "    \n",
    "    # share drug name, but linkers are different\n",
    "    if shared_drug != [] and s1_features != s2_features:\n",
    "        return 1\n",
    "    else: \n",
    "        return 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "edc1bb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# given s1, s2 Sentence instances\n",
    "def get_feature_df(s1, s2, label, pair_id, type_data):\n",
    "    sentence_1 = s1.txt\n",
    "    sentence_2 = s2.txt\n",
    "    sentence_df = create_sentence_encoding(sentence_1, sentence_2)\n",
    "    \n",
    "    # check negation in docs\n",
    "    neg_check_umls = check_if_number_neg_equal_umls(s1, s2)\n",
    "    sentence_df[\"neg_check_umls\"] = neg_check_umls\n",
    "\n",
    "    # check shared features in UMLS and Med7\n",
    "    check_umls = check_shared_feature_umls(s1,s2)\n",
    "    check_med7 = check_shared_feature_med7(s1,s2)\n",
    "    sentence_df[\"check_umls\"] = check_umls\n",
    "    sentence_df[\"check_med7\"] = check_med7\n",
    "    \n",
    "    # find dependent children -> BOW\n",
    "    #dep_sent_1, dep_sent_2 = create_dep_encoding(s1, s2)\n",
    "    #dep_df = create_sentence_encoding(dep_sent_1, dep_sent_2)\n",
    "    #print(dep_df)\n",
    "    #feature_df = pd.concat((sentence_df, dep_df), axis=1)\n",
    "    dep_similarity = create_dep_encoding(s1, s2)\n",
    "    sentence_df[\"dep_sim\"] = dep_similarity\n",
    "    \n",
    "    # put contradiction label\n",
    "    sentence_df[\"contradiction?\"] = label\n",
    "    \n",
    "    # put pair_id for reference\n",
    "    sentence_df[\"pair_id\"] = pair_id\n",
    "    sentence_df[\"type_data\"] = type_data\n",
    "    return sentence_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "69201fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/scispacy/candidate_generation.py:284: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  extended_neighbors[empty_vectors_boolean_flags] = numpy.array(neighbors)[:-1]\n",
      "/opt/conda/lib/python3.7/site-packages/scispacy/candidate_generation.py:285: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  extended_distances[empty_vectors_boolean_flags] = numpy.array(distances)[:-1]\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:35: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>baseline</th>\n",
       "      <th>cr</th>\n",
       "      <th>elevated</th>\n",
       "      <th>labs</th>\n",
       "      <th>lactate</th>\n",
       "      <th>notable</th>\n",
       "      <th>old</th>\n",
       "      <th>patient</th>\n",
       "      <th>records</th>\n",
       "      <th>neg_check_umls</th>\n",
       "      <th>check_umls</th>\n",
       "      <th>check_med7</th>\n",
       "      <th>dep_sim</th>\n",
       "      <th>contradiction?</th>\n",
       "      <th>pair_id</th>\n",
       "      <th>type_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.611129</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.611129</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   baseline  cr  elevated  labs  lactate  notable  old  patient  records  \\\n",
       "0         1   1         0     1        1        1    1        0        1   \n",
       "1         0   1         1     0        0        0    0        1        0   \n",
       "\n",
       "   neg_check_umls  check_umls  check_med7   dep_sim  contradiction?  pair_id  \\\n",
       "0               0           0           0  0.611129               0        0   \n",
       "1               0           0           0  0.611129               0        0   \n",
       "\n",
       "  type_data  \n",
       "0     train  \n",
       "1     train  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_feature_df(s1, s2, 0, 0, \"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8046c8",
   "metadata": {},
   "source": [
    "## Scale up features to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f5967846",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7488"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ffb944d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7488 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/scispacy/candidate_generation.py:284: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  extended_neighbors[empty_vectors_boolean_flags] = numpy.array(neighbors)[:-1]\n",
      "/opt/conda/lib/python3.7/site-packages/scispacy/candidate_generation.py:285: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  extended_distances[empty_vectors_boolean_flags] = numpy.array(distances)[:-1]\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:35: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "100%|██████████| 7488/7488 [1:52:17<00:00,  1.11it/s]\n"
     ]
    }
   ],
   "source": [
    "total_features_df = pd.DataFrame()\n",
    "\n",
    "for pair_id in tqdm(range(len(train_dataset))):\n",
    "    \n",
    "    (s1, s2), label = train_dataset[pair_id]\n",
    "    #print(s1.txt)\n",
    "    #print(s2.txt)\n",
    "    #print(label)\n",
    "    type_data = \"train\"\n",
    "    feature_df = get_feature_df(s1, s2, label, pair_id, type_data)\n",
    "    total_features_df = pd.concat((total_features_df, feature_df), axis=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "60a7089c",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_features_df.to_csv(\"train_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f3e53e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/948 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:35: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "100%|██████████| 948/948 [06:33<00:00,  2.41it/s]\n"
     ]
    }
   ],
   "source": [
    "#test_file  = \"mednli_labeled/test.csv\"\n",
    "\n",
    "test_dataset = MedNLI(test_file)\n",
    "\n",
    "test_total_features_df = pd.DataFrame()\n",
    "\n",
    "for pair_id in tqdm(range(len(test_dataset))):\n",
    "    (s1, s2), label = test_dataset[pair_id]\n",
    "    type_data = \"test\"\n",
    "    feature_df = get_feature_df(s1, s2, label, pair_id, type_data)\n",
    "    test_total_features_df = pd.concat((test_total_features_df, feature_df), axis=0)\n",
    "    \n",
    "test_total_features_df.to_csv(\"test_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e7f0378d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/930 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:35: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "100%|██████████| 930/930 [06:20<00:00,  2.44it/s]\n"
     ]
    }
   ],
   "source": [
    "#dev_file   = \"mednli_labeled/dev.csv\"\n",
    "\n",
    "dev_dataset = MedNLI(dev_file)\n",
    "\n",
    "dev_total_features_df = pd.DataFrame()\n",
    "\n",
    "for pair_id in tqdm(range(len(dev_dataset))):\n",
    "    (s1, s2), label = dev_dataset[pair_id]\n",
    "    type_data = \"dev\"\n",
    "    feature_df = get_feature_df(s1, s2, label, pair_id, type_data)\n",
    "    dev_total_features_df = pd.concat((test_total_features_df, feature_df), axis=0)\n",
    "    \n",
    "dev_total_features_df.to_csv(\"dev_features.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f9519a",
   "metadata": {},
   "source": [
    "## Hybrid rule based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fe099e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RuleAugmentedEstimator(BaseEstimator):\n",
    "    \"\"\"\n",
    "    Augments sklearn estimators with deterministic rule-based logic.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_model: BaseEstimator, rules: Dict, **base_params):\n",
    "        \"\"\"\n",
    "        Initializes the rule-augmented estimator by supplying underlying sklearn estimator\n",
    "        and hard-coded rules.\n",
    "\n",
    "        Args:\n",
    "        base_model: underlying sklearn estimator.\n",
    "          Must implement fit and predict method.\n",
    "        rules: hard coded rules in format of dictionary,\n",
    "          with keys being the pandas dataframe column name, \n",
    "          and values being a tuple in the following form: \n",
    "          (comparison operator, value, return value)\n",
    "\n",
    "          Acceptable comparison operators are: \n",
    "          \"=\", \"<\", \">\", \"<=\", \">=\"\n",
    "\n",
    "          Example:\n",
    "\n",
    "                {\"House Type\": [\n",
    "                    (\"=\", \"Penthouse\", 1.0),\n",
    "                    (\"=\", \"Shack\", 0.0)\n",
    "                  ],\n",
    "                  \"House Price\": [\n",
    "                      (\"<\", 1000.0, 0.0),\n",
    "                      (\">=\", 500000.0, 1.0)\n",
    "                ]}\n",
    "        **base_params: Optional keyword arguments which will be passed on\n",
    "            to the base_model.\n",
    "\n",
    "        \"\"\"\n",
    "        self.rules = rules\n",
    "        self.base_model = base_model\n",
    "        self.base_model.set_params(**base_params)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Rule Augmented Estimator:\\n\\n\\t Base Model: {}\\n\\t Rules: {}\".format(self.base_model, self.rules)\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.__str__\n",
    "\n",
    "    def _get_base_model_data(self, X: pd.DataFrame, y: pd.Series) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "        \"\"\"\n",
    "        Filters the training data for data points not affected by the rules.\n",
    "        \"\"\"\n",
    "        train_x = X\n",
    "\n",
    "        for category, rules in self.rules.items():\n",
    "\n",
    "            if category not in train_x.columns.values: continue\n",
    "\n",
    "            for rule in rules:\n",
    "\n",
    "                if rule[0] == \"=\":\n",
    "                    train_x = train_x.loc[train_x[category] != rule[1]]\n",
    "\n",
    "                elif rule[0] == \"<\":\n",
    "                    train_x = train_x.loc[train_x[category] >= rule[1]]\n",
    "\n",
    "                elif rule[0] == \">\":\n",
    "                    train_x = train_x.loc[train_x[category] <= rule[1]]\n",
    "\n",
    "                elif rule[0] == \"<=\":\n",
    "                    train_x = train_x.loc[train_x[category] > rule[1]]\n",
    "\n",
    "                elif rule[0] == \">=\":\n",
    "                    train_x = train_x.loc[train_x[category] < rule[1]]\n",
    "\n",
    "                else:\n",
    "                    #print(\"Invalid rule detected: {}\".format(rule))\n",
    "                    continue\n",
    "\n",
    "        indices = train_x.index.values\n",
    "        train_y = y.iloc[indices]\n",
    "\n",
    "        train_x = train_x.reset_index(drop=True)\n",
    "        train_y = train_y.reset_index(drop=True)\n",
    "\n",
    "        return train_x, train_y   \n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y: pd.Series, **kwargs):\n",
    "        \"\"\"Fits the estimator to the data.\n",
    "\n",
    "        Fits the estimator to the data, only training the underlying estimator\n",
    "        on data which isn't affected by the hard-coded rules.\n",
    "\n",
    "        Args:\n",
    "          X: The training feature data.\n",
    "          y: The training label data.\n",
    "          **kwargs: Optional keyword arguments passed to the underlying\n",
    "          estimator's fit function.\n",
    "\n",
    "        \"\"\"\n",
    "        train_x, train_y = self._get_base_model_data(X, y)\n",
    "        self.base_model.fit(train_x, train_y, **kwargs)\n",
    "\n",
    "    def predict(self, X: pd.DataFrame) -> np.array:\n",
    "        \"\"\"Gets predictions for the provided feature data.\n",
    "\n",
    "        The predicitons are evaluated using the provided rules wherever possible\n",
    "        otherwise the underlying estimator is used.\n",
    "\n",
    "        Args:\n",
    "          X: The feature data to evaluate predictions for.\n",
    "\n",
    "        Returns:\n",
    "          np.array: Evaluated predictions.\n",
    "        \"\"\"\n",
    "      \n",
    "        p_X = X.copy()\n",
    "        p_X['prediction'] = np.nan\n",
    "\n",
    "        for category, rules in self.rules.items():\n",
    "\n",
    "            if category not in p_X.columns.values: continue\n",
    "\n",
    "            for rule in rules:\n",
    "\n",
    "            if rule[0] == \"=\":\n",
    "                p_X.loc[p_X[category] == rule[1], 'prediction'] = rule[2]\n",
    "\n",
    "            elif rule[0] == \"<\":\n",
    "                p_X.loc[p_X[category] < rule[1], 'prediction'] = rule[2]\n",
    "\n",
    "            elif rule[0] == \">\":\n",
    "                p_X.loc[p_X[category] > rule[1], 'prediction'] = rule[2]\n",
    "\n",
    "            elif rule[0] == \"<=\":\n",
    "                p_X.loc[p_X[category] <= rule[1], 'prediction'] = rule[2]\n",
    "\n",
    "            elif rule[0] == \">=\":\n",
    "                p_X.loc[p_X[category] >= rule[1], 'prediction'] = rule[2]\n",
    "\n",
    "            else:\n",
    "                #print(\"Invalid rule detected: {}\".format(rule))\n",
    "                continue\n",
    "\n",
    "        if len(p_X.loc[p_X['prediction'].isna()].index != 0):\n",
    "\n",
    "            base_X = p_X.loc[p_X['prediction'].isna()].copy()\n",
    "            base_X.drop('prediction', axis=1, inplace=True)\n",
    "            p_X.loc[p_X['prediction'].isna(), 'prediction'] = self.base_model.predict(base_X)\n",
    "\n",
    "        return p_X['prediction'].values\n",
    "    \n",
    "    def get_params(self, deep: bool = True) -> Dict:\n",
    "        \"\"\"Return the model's and base model's parameters.\n",
    "        Args:\n",
    "            deep: Whether to recursively return the base model's parameters.\n",
    "        Returns\n",
    "            Dict: The model's parameters.\n",
    "        \"\"\"\n",
    "        \n",
    "        params = {'base_model': self.base_model,\n",
    "                  'outcome_range': self.outcome_range,\n",
    "                  'rules': self.rules\n",
    "                  }\n",
    "\n",
    "        params.update(self.base_model.get_params(deep=deep))\n",
    "        return params\n",
    "    \n",
    "    def set_params(self, **params):\n",
    "        \"\"\"Sets parameters for the model and base model.\n",
    "        Args:\n",
    "          **params: Optional keyword arguments.\n",
    "        \"\"\"\n",
    "                \n",
    "        parameters = params\n",
    "        param_keys = parameters.keys()\n",
    "\n",
    "        if 'base_model' in param_keys:\n",
    "            value = parameters.pop('base_model')\n",
    "            self.base_model = value\n",
    "\n",
    "        if 'rules' in param_keys:\n",
    "            value = parameters.pop('rules')\n",
    "            self.rules = value\n",
    "\n",
    "        self.base_model.set_params(**parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322bbf1f",
   "metadata": {},
   "source": [
    "## Setup rules and train, dev, test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de61d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RULES:\n",
    "# check_umls: if they do not share a concept, return 1 (no contradiction)\n",
    "# neg_check_umls: if number of neg tokens is equal, return 0 (no contradiction); otherwise, return 1 (contradiction)\n",
    "# check_med7: if not talking about same DRUG -> return 0 (no contradiction); if same DRUG but different other info -> return 1 (contradiction)\n",
    "# dep_sim: if number < 0.5 (contradiction)\n",
    "\n",
    "rules = {\"check_umls\": [\n",
    "                (\"=\", 1, 0.0)\n",
    "              ],\n",
    "        \"neg_check_umls\": [\n",
    "                (\"=\", 0, 0.0),\n",
    "                (\"=\", 1, 1.0)\n",
    "              ],\n",
    "        \"check_med7\": [\n",
    "                (\"=\", 0, 0.0),\n",
    "                (\"=\", 1, 1.0)\n",
    "              ],\n",
    "              \n",
    "        \"dep_sim\": [\n",
    "                (\"<\", 0.5, 1.0)\n",
    "              ]\n",
    "        }\n",
    "\n",
    "\n",
    "# split labeled data into training, testing sets\n",
    "train_df = pd.read_csv(\"train_features.csv\")\n",
    "dev_df = pd.read_csv(\"dev_features.csv\")\n",
    "test_df = pd.read_csv(\"test_features.csv\")\n",
    "\n",
    "train_X = train_df.drop(['pair_id', 'type_data'], axis=1)\n",
    "train_y = train_df[\"contradiction?\"]\n",
    "\n",
    "dev_X = dev_df.drop(['pair_id', 'type_data'], axis=1)\n",
    "dev_y = dev_df[\"contradiction?\"]\n",
    "\n",
    "train_X = test_df.drop(['pair_id', 'type_data'], axis=1)\n",
    "train_y = test_df[\"contradiction?\"]\n",
    "\n",
    "# handle NaNs\n",
    "\n",
    "#train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd77872",
   "metadata": {},
   "source": [
    "## Training loop and hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fda3a9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b053a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# supplement a decision tree algorithm with rules\n",
    "\n",
    "gbc = GradientBoostingClassifier(n_estimators=50)\n",
    "hybrid_model = RuleAugmentedEstimator(gbc, rules)\n",
    "hybrid_model.fit(train_X, train_y)\n",
    "predictions = hybrid_model.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7009d20a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d34549a3",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5ba723",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3886a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no skill prediction \n",
    "ns_probs = [0 for _ in range(len(test_y))]\n",
    "\n",
    "# predict probabilities\n",
    "gbc_probs = gbc.predict_proba(train_X)\n",
    "gbc_probs = gbc_probs[:, 1]\n",
    "\n",
    "# calculate scores\n",
    "ns_auc = roc_auc_score(test_y, ns_probs)\n",
    "gbc_auc = roc_auc_score(test_y, gbc_probs)\n",
    "print('No Skill: ROC AUC=%.3f' % (ns_auc))\n",
    "print('GBC AUC:' % gbc_auc)\n",
    "\n",
    "# calculate roc curve\n",
    "ns_fpr, ns_tpr, _ = roc_curve(testy, ns_probs)\n",
    "gbc_fpr, gbc_tpr, _ = roc_curve(test_y, gbc_probs)\n",
    "\n",
    "# plot roc curve for model\n",
    "pyplot.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n",
    "pyplot.plot(lr_fpr, lr_tpr, marker='.', label='Logistic')\n",
    "\n",
    "# axis labels\n",
    "pyplot.xlabel('False Positive Rate')\n",
    "pyplot.ylabel('True Positive Rate')\n",
    "# show the legend\n",
    "pyplot.legend()\n",
    "# show the plot\n",
    "pyplot.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc9eb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Evaluation metrics (precision/PPV, recall/sensitivity, f1-score, support)\n",
    "\"\"\"\n",
    "\n",
    "y_true = test_y\n",
    "y_pred = predictions\n",
    "\n",
    "# label = 0 if no contradiction, 1 if there is a contradiction\n",
    "print(classification_report(y_true, y_pred, labels=[0,1]))\n",
    "\n",
    "#target_names = ['no contradiction', 'contradiction']\n",
    "#print(classification_report(y_true, y_pred, target_names=target_names))\n",
    "\n",
    "# AUC\n",
    "fpr, tpr, thresholds = roc_curve(y, probs)\n",
    "auc = roc_auc_score(y, probs)\n",
    "print('AUC: %.3f' % auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902450b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
